{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bc3GWn_jojsG"
   },
   "source": [
    "# MNIST notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXpH5753pfZU"
   },
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DG-wTTncnnjX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 04:23:33.453227: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-22 04:23:33.486707: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-22 04:23:33.496403: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Set TensorFlow logging to only show errors\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # This silences INFO and WARNING messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Add, Dense, Dropout, Embedding, GlobalAveragePooling1D, Input, Layer, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "try:\n",
    "  import pennylane as qml\n",
    "except:\n",
    "  !pip install pennylane\n",
    "  import pennylane as qml\n",
    "from pennylane.operation import Operation\n",
    "import random as rd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oa2gttKppTU"
   },
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Vr_vMdH-FlP",
    "outputId": "3238e9ed-36db-40d5-a7b8-c5cc0917030c"
   },
   "outputs": [],
   "source": [
    "#set random seed\n",
    "random = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXdj0ZilYdII",
    "outputId": "469ad37f-d244-40ff-b1d2-4d3c86e63bce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train labels: (500, 2)\n",
      "Shape of test labels: (300, 2)\n"
     ]
    }
   ],
   "source": [
    "# Expand the dimensions of the images to (28, 28, 1) to represent the grayscale channel explicitly\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Filter for only the classes 0 and 3\n",
    "train_filter = np.where((y_train == 0) | (y_train == 3))\n",
    "test_filter = np.where((y_test == 0) | (y_test == 3))\n",
    "\n",
    "x_train, y_train = x_train[train_filter], y_train[train_filter]\n",
    "x_test, y_test = x_test[test_filter], y_test[test_filter]\n",
    "\n",
    "# Use the first 500 images as the training set\n",
    "x_train, y_train = x_train[:500], y_train[:500]\n",
    "# Randomly sample 300 images from the remaining data as the validation set\n",
    "x_val, y_val = x_test[500:800], y_test[500:800]\n",
    "\n",
    "x_test, y_test = x_test[:300], y_test[:300]\n",
    "\n",
    "# Rescale the images\n",
    "x_train = x_train / 255.0\n",
    "x_val = x_val / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "train_images = np.expand_dims(x_train, -1)\n",
    "test_images = np.expand_dims(x_test, -1)\n",
    "\n",
    "def preprocess_images(images):\n",
    "    # Center-crop to 24x24\n",
    "    images_cropped = tf.image.central_crop(images, central_fraction=24/28)\n",
    "    \n",
    "    # Down-sample to 4x4\n",
    "    images_downsampled = tf.image.resize(images_cropped, size=(4, 4), method=tf.image.ResizeMethod.BILINEAR)\n",
    "    \n",
    "    return images_downsampled\n",
    "\n",
    "# Preprocess the train, validation, and test images\n",
    "train_images = preprocess_images(train_images)\n",
    "test_images= preprocess_images(test_images)\n",
    "\n",
    "# Map the labels 3 -> 0 and 6 -> 1\n",
    "y_train_binary = np.where(y_train == 3, 0, 1)\n",
    "y_test_binary = np.where(y_test == 3, 0, 1)\n",
    "\n",
    "# Convert the labels to one-hot encoded vectors\n",
    "train_labels = to_categorical(y_train_binary, 2)\n",
    "test_labels = to_categorical(y_test_binary, 2)\n",
    "\n",
    "print(\"Shape of train labels:\", train_labels.shape)\n",
    "print(\"Shape of test labels:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DQ4cCjAJqaIZ"
   },
   "outputs": [],
   "source": [
    "def plot_images(images, labels, num_images=25, figsize=(10,10)):\n",
    "    grid_size = 5\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(grid_size, grid_size, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i], cmap='gray')\n",
    "        plt.xlabel(f'Label: {labels[i]}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(history):\n",
    "    # Extracting training and validation accuracy\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    # Extracting training and validation loss\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Plotting the accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(accuracy, label='Training Accuracy')\n",
    "    plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ryf3e3L6GKj"
   },
   "source": [
    "## Common Quantum functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K7mK4jvaa_RS"
   },
   "outputs": [],
   "source": [
    "class RBSGate(Operation):\n",
    "    num_params = 1\n",
    "    num_wires = 2\n",
    "    par_domain = 'R'\n",
    "\n",
    "    def __init__(self, theta, wires):\n",
    "        super().__init__(theta, wires=wires)\n",
    "        self.theta = theta\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_matrix(theta):\n",
    "        cos = tf.cos(theta)\n",
    "        sin = tf.sin(theta)\n",
    "        return tf.convert_to_tensor([\n",
    "            [1, 0, 0, 0],\n",
    "            [0, cos, sin, 0],\n",
    "            [0, -sin, cos, 0],\n",
    "            [0, 0, 0, 1]\n",
    "        ], dtype=tf.float64)\n",
    "\n",
    "    def adjoint(self):\n",
    "        return RBSGate(-self.parameters[0], wires=self.wires)\n",
    "\n",
    "    def label(self, decimals=None, base_label=None, **kwargs):\n",
    "        theta = self.parameters[0]\n",
    "        return f\"RBS({theta:.2f})\"\n",
    "def convert_array(X):\n",
    "    alphas = tf.zeros(X.shape[:-1] + (X.shape[-1]-1,), dtype=X.dtype)\n",
    "    X_normd = tf.linalg.l2_normalize(X, axis=-1)\n",
    "    for i in range(X.shape[-1]-1):\n",
    "        prod_sin_alphas = tf.reduce_prod(tf.sin(alphas[..., :i]), axis=-1)\n",
    "        updated_value = tf.acos(X_normd[..., i] / prod_sin_alphas)\n",
    "        indices = tf.constant([[i]])\n",
    "        updates = tf.reshape(updated_value, [1])\n",
    "        alphas = tf.tensor_scatter_nd_update(alphas, indices, updates)\n",
    "    return alphas\n",
    "def vector_loader(alphas, wires=None, is_x=True, is_conjugate=False):\n",
    "    if wires is None:\n",
    "        wires = list(range(len(alphas) + 1))\n",
    "    if is_x and not is_conjugate:\n",
    "        qml.PauliX(wires=wires[0])\n",
    "    if is_conjugate:\n",
    "        for i in range(len(wires) - 2, -1, -1):\n",
    "            qml.apply(RBSGate(-alphas[i], wires=[wires[i], wires[i+1]]))\n",
    "    else:\n",
    "        for i in range(len(wires) - 1):\n",
    "            qml.apply(RBSGate(alphas[i], wires=[wires[i], wires[i+1]]))\n",
    "    if is_x and is_conjugate:\n",
    "        qml.PauliX(wires=wires[0])\n",
    "def pyramid_circuit(parameters, wires=None):\n",
    "    if wires is None:\n",
    "        length = len(qml.device.wires)\n",
    "    else:\n",
    "        length = len(wires)\n",
    "\n",
    "    k = 0\n",
    "\n",
    "    for i in range(2 * length - 2):\n",
    "        j = length - abs(length - 1 - i)\n",
    "\n",
    "        if i % 2:\n",
    "            for _ in range(j):\n",
    "                if _ % 2 == 0 and k < (parameters.shape[0]):\n",
    "                    qml.apply(RBSGate(parameters[k], wires=([wires[_], wires[_ + 1]])))\n",
    "                    k += 1\n",
    "        else:\n",
    "            for _ in range(j):\n",
    "                if _ % 2 and k < (parameters.shape[0]):\n",
    "                    qml.apply(RBSGate(parameters[k], wires=([wires[_], wires[_ + 1]])))\n",
    "                    k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caklx0-k6RHm"
   },
   "source": [
    "# qOrthNN + Gradient Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Define a step counter\n",
    "global_step = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
    "\n",
    "# Define a custom callback\n",
    "class StepCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        global_step.assign(0)\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        global_step.assign_add(1)\n",
    "        current_step = global_step.numpy()\n",
    "        if current_step % 5 == 0:\n",
    "            # Increase the pruning ratio\n",
    "            current_prune_ratio = self.model.prune_ratio\n",
    "            # Increase prune_ratio by multiplying with exp(0.1)\n",
    "            new_prune_ratio = min(current_prune_ratio * math.exp(0.1), 1.0)  # Cap at 1.0\n",
    "            self.model.prune_ratio = new_prune_ratio\n",
    "    # def on_epoch_begin(self, epoch, logs=None):\n",
    "    #     tf.print(\"Updated prune_ratio to\", self.model.prune_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6AXTsrKB6Uui"
   },
   "outputs": [],
   "source": [
    "class HybridModel(tf.keras.Model):\n",
    "    def __init__(self,random1):\n",
    "        super(HybridModel, self).__init__()\n",
    "        \n",
    "        rd.seed(random1)\n",
    "        np.random.seed(random1)\n",
    "        tf.random.set_seed(random1)\n",
    "        qml.numpy.random.seed(random1)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(6, activation='linear', dtype=tf.float64)\n",
    "        self.quantum_weights = self.add_weight(\n",
    "            shape=(15,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        self.dev = qml.device('default.qubit.tf', wires=6)\n",
    "        # Pruning attributes\n",
    "        self.accumulated_grads = tf.Variable(tf.zeros_like(self.quantum_weights), trainable=False)\n",
    "        self.accumulate_flag = tf.Variable(True, trainable=False)\n",
    "        \n",
    "        self.accumulate_window = tf.constant(10)\n",
    "        self.prune_window = tf.constant(6)\n",
    "        self.prune_ratio = tf.constant(0.)\n",
    "        \n",
    "        self.accumulate_count = tf.Variable(6, dtype=tf.int32, trainable=False)\n",
    "        self.prune_count = tf.Variable(6, dtype=tf.int32, trainable=False)\n",
    "\n",
    "\n",
    "        @qml.qnode(self.dev, interface='tf', diff_method='backprop')\n",
    "        def quantum_circuit(inputs, weights):\n",
    "            inputs = tf.cast(inputs, tf.float32)\n",
    "            weights = tf.cast(weights, tf.float32)\n",
    "            vector_loader(convert_array(inputs), wires=range(6))\n",
    "            pyramid_circuit(weights, wires=range(6))\n",
    "            return [qml.expval(qml.PauliZ(wire)) for wire in range(6)]\n",
    "\n",
    "        self.quantum_circuit = quantum_circuit\n",
    "        self.classical_nn_2 = tf.keras.layers.Dense(2, activation='sigmoid', dtype=tf.float64)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.cast(inputs, tf.float64)\n",
    "        flattened_inputs = self.flatten(inputs)\n",
    "        classical_output = self.dense(flattened_inputs)\n",
    "        quantum_outputs = tf.map_fn(\n",
    "            lambda x: tf.stack(self.quantum_circuit(x, self.quantum_weights)),\n",
    "            classical_output,\n",
    "            fn_output_signature=tf.TensorSpec(shape=(6,), dtype=tf.float64)\n",
    "        )\n",
    "        # Handle NaN values in quantum outputs\n",
    "        quantum_outputs = tf.where(tf.math.is_nan(quantum_outputs), tf.zeros_like(quantum_outputs), quantum_outputs)\n",
    "\n",
    "        # Combine and process quantum outputs through additional NN layers\n",
    "        quantum_outputs = tf.reshape(quantum_outputs, [-1, 6])\n",
    "        nn_output = self.classical_nn_2(quantum_outputs)\n",
    "\n",
    "        return nn_output\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        x, y = data  # Unpack the data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "\n",
    "        # Index of quantum_weights in trainable_variables\n",
    "        quantum_weights_idx = 0\n",
    "        \n",
    "        # switch flag for PGP\n",
    "        @tf.function\n",
    "        def switch_flag():\n",
    "            if self.accumulate_count == 0:\n",
    "                self.accumulate_count.assign(self.accumulate_window)\n",
    "                self.accumulate_flag.assign(False)\n",
    "            elif self.prune_count == 0:\n",
    "                self.prune_count.assign(self.prune_window)\n",
    "                self.accumulate_flag.assign(True)\n",
    "        switch_flag()\n",
    "        \n",
    "        # Probabilistic Gradient Pruning\n",
    "        if tf.equal(self.accumulate_flag, True):\n",
    "            # Step 1: Accumulate gradients for quantum_weights\n",
    "            # tf.print(\"Accu phase:\", self.accumulate_count)\n",
    "            self.accumulate_count.assign_sub(1)\n",
    "            if gradients[quantum_weights_idx] is not None:\n",
    "                self.accumulated_grads.assign_add(gradients[quantum_weights_idx])\n",
    "\n",
    "            # Apply gradients for other variables (excluding quantum_weights)\n",
    "            # other_gradients = []\n",
    "            # other_variables = []\n",
    "            # for i, (grad, var) in enumerate(zip(gradients, self.trainable_variables)):\n",
    "            #     if i != quantum_weights_idx and grad is not None:\n",
    "            #         other_gradients.append(grad)\n",
    "            #         other_variables.append(var)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        else:\n",
    "            # Step 2: Prune weights of quantum_weights\n",
    "            # tf.print(\"Pruning phase:\",self.prune_count)\n",
    "            self.prune_count.assign_sub(1)\n",
    "            # Normalize the accumulated gradients\n",
    "            grad_min = tf.reduce_min(self.accumulated_grads)\n",
    "            grad_max = tf.reduce_max(self.accumulated_grads)\n",
    "            epsilon = 1e-8\n",
    "            norm_grads = (self.accumulated_grads - grad_min) / (grad_max - grad_min + epsilon)\n",
    "\n",
    "            # Add epsilon to norm_grads to avoid log(0)\n",
    "            norm_grads_with_epsilon = norm_grads + epsilon\n",
    "\n",
    "            # Compute logits for categorical sampling\n",
    "            logits = tf.math.log(norm_grads_with_epsilon)\n",
    "\n",
    "            # Determine the number of parameters to sample\n",
    "            num_params = self.quantum_weights.shape[0]\n",
    "            num_samples = int(self.prune_ratio * num_params)\n",
    "            num_samples = tf.maximum(1, num_samples) # Ensure at least one parameter is sampled\n",
    "\n",
    "            # Sample indices based on the normalized gradients\n",
    "            indices = tf.random.categorical([logits], num_samples=num_samples)\n",
    "            indices = tf.clip_by_value(indices, 0, self.quantum_weights.shape[0] - 1)  # Ensure indices are within range\n",
    "\n",
    "            # Create a boolean mask to select the parameters to keep\n",
    "            mask = tf.zeros_like(self.quantum_weights, dtype=tf.bool)\n",
    "            indices = tf.cast(indices, tf.int32)\n",
    "            indices = tf.reshape(indices, [-1, 1])  # Ensure indices are shaped correctly\n",
    "            updates = tf.ones([tf.shape(indices)[0]], dtype=tf.bool)  # Create updates matching the indices length\n",
    "\n",
    "            mask = tf.tensor_scatter_nd_update(mask, indices, updates)\n",
    "\n",
    "            # Apply the mask to the accumulated gradients\n",
    "            pruned_grad = tf.where(mask, self.accumulated_grads[0], tf.zeros_like(self.accumulated_grads[0]))\n",
    "\n",
    "            # Apply the pruned gradient to quantum_weights\n",
    "            self.optimizer.apply_gradients([(pruned_grad, self.quantum_weights)])\n",
    "\n",
    "            # Apply gradients for other variables (excluding quantum_weights)\n",
    "            other_gradients = []\n",
    "            other_variables = []\n",
    "            for i, (grad, var) in enumerate(zip(gradients, self.trainable_variables)):\n",
    "                if i != quantum_weights_idx and grad is not None:\n",
    "                    other_gradients.append(grad)\n",
    "                    other_variables.append(var)\n",
    "            self.optimizer.apply_gradients(zip(other_gradients, other_variables))\n",
    "\n",
    "            # Reset accumulator and accumulate window\n",
    "            self.accumulated_grads.assign(tf.zeros_like(self.accumulated_grads))\n",
    "\n",
    "        # Sanitize weights: replace NaNs with zeros\n",
    "        for var in self.trainable_variables:\n",
    "            # Create a mask where NaNs are present\n",
    "            nan_mask = tf.math.is_nan(var)\n",
    "            # Replace NaNs with zeros\n",
    "            sanitized_var = tf.where(nan_mask, tf.zeros_like(var), var)\n",
    "            # Assign the sanitized variable back to the model\n",
    "            var.assign(sanitized_var)\n",
    "\n",
    "        # Update metrics\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # tf.print(self.quantum_weights)\n",
    "        # Return a dictionary of metric results\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Hypothesis (Monte Carlo inspection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Accuracy: 0.8747\n",
      "95% Confidence Interval for Accuracy: [0.8589, 0.8852]\n",
      "Estimated AUC: 0.9392\n",
      "95% Confidence Interval for AUC: [0.9282, 0.9470]\n"
     ]
    }
   ],
   "source": [
    "rd.seed(random)\n",
    "np.random.seed(random)\n",
    "tf.random.set_seed(random)\n",
    "qml.numpy.random.seed(random)\n",
    "model = HybridModel(random)\n",
    "# Cosine learning rate scheduler from 0.3 to 0.03\n",
    "initial_learning_rate = 0.3\n",
    "final_learning_rate = 0.03\n",
    "alpha = final_learning_rate / initial_learning_rate\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=16,\n",
    "    alpha=alpha\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, \n",
    "            loss='binary_crossentropy', \n",
    "            metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "# Number of Monte Carlo simulations\n",
    "n_simulations = 10\n",
    "\n",
    "# Container for results\n",
    "accuracies = []\n",
    "aucs = []\n",
    "\n",
    "# Monte Carlo Resampling\n",
    "for _ in range(n_simulations):\n",
    "    # # Resample train/test data (e.g., 70% training, 30% testing)\n",
    "    # indices = np.arange(len(train_images))\n",
    "    # np.random.shuffle(indices)\n",
    "    # split_idx = int(0.7 * len(train_images))\n",
    "    # train_indices = indices[:split_idx]\n",
    "    # test_indices = indices[split_idx:]\n",
    "    \n",
    "    # X_train, y_train = train_images[train_indices], train_labels[train_indices]\n",
    "    # X_test, y_test = train_images[test_indices], train_labels[test_indices]\n",
    "\n",
    "    # Train the model on resampled data\n",
    "    model.fit(train_images, train_labels, \n",
    "            epochs=10,  # Fewer epochs for faster resampling\n",
    "            batch_size=32, \n",
    "            verbose=0)  # Suppress verbose output\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy, test_auc = model.evaluate(test_images, test_labels, verbose=0)\n",
    "    \n",
    "    # Store the accuracy and AUC\n",
    "    accuracies.append(test_accuracy)\n",
    "    aucs.append(test_auc)\n",
    "\n",
    "# Calculate average metrics and 95% confidence intervals      mddg  sfsdgfgdfffg\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "ci_accuracy_lower = np.percentile(accuracies, 2.5)\n",
    "ci_accuracy_upper = np.percentile(accuracies, 97.5)\n",
    "\n",
    "mean_auc = np.mean(aucs)\n",
    "ci_auc_lower = np.percentile(aucs, 2.5)\n",
    "ci_auc_upper = np.percentile(aucs, 97.5)\n",
    "\n",
    "# Print results\n",
    "print(f\"Estimated Accuracy: {mean_accuracy:.4f}\")\n",
    "print(f\"95% Confidence Interval for Accuracy: [{ci_accuracy_lower:.4f}, {ci_accuracy_upper:.4f}]\")\n",
    "print(f\"Estimated AUC: {mean_auc:.4f}\")\n",
    "print(f\"95% Confidence Interval for AUC: [{ci_auc_lower:.4f}, {ci_auc_upper:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "rd.seed(random)\n",
    "np.random.seed(random)\n",
    "tf.random.set_seed(random)\n",
    "qml.numpy.random.seed(random)\n",
    "print('Training with random seed:', random)\n",
    "model = HybridModel(random)\n",
    "total_steps = 16  # Example value; adjust based on your training setup\n",
    "\n",
    "# Cosine learning rate scheduler from 0.3 to 0.03\n",
    "initial_learning_rate = 0.3\n",
    "final_learning_rate = 0.03\n",
    "alpha = final_learning_rate / initial_learning_rate\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=total_steps,\n",
    "    alpha=alpha\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',   # Metric to monitor\n",
    "    patience=3,           # Number of epochs with no improvement after which training will be stopped\n",
    "    min_delta=0.1,      # Minimum change to qualify as an improvement\n",
    "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored metric\n",
    ")\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model and store the validation accuracy\n",
    "history = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=8,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    verbose=1,\n",
    "    callbacks=[StepCallback()]  # Include the EarlyStopping callback\n",
    ")\n",
    "\n",
    "print('Testing Process')\n",
    "test_loss, test_accuracy  = model.evaluate(test_images, test_labels)\n",
    "accuracy.append(test_accuracy)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "plot_learning_curve(history)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "uXpH5753pfZU"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Pennylane",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
