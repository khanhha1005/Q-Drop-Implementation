{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc3GWn_jojsG"
      },
      "source": [
        "# Pneumonia MNIST notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXpH5753pfZU"
      },
      "source": [
        "## import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DG-wTTncnnjX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-22 00:44:44.463433: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-22 00:44:44.485076: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-22 00:44:44.491465: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Set TensorFlow logging to only show errors\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # This silences INFO and WARNING messages\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Add, Dense, Dropout, Embedding, GlobalAveragePooling1D, Input, Layer, LayerNormalization, MultiHeadAttention\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "try:\n",
        "  import pennylane as qml\n",
        "except:\n",
        "  !pip install pennylane\n",
        "  import pennylane as qml\n",
        "from pennylane.operation import Operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oa2gttKppTU"
      },
      "source": [
        "## Import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Vr_vMdH-FlP",
        "outputId": "3238e9ed-36db-40d5-a7b8-c5cc0917030c"
      },
      "outputs": [],
      "source": [
        "# !pip install medmnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t9VleEId-FW2"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import medmnist\n",
        "from medmnist import INFO, Evaluator\n",
        "import random as rd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP2uhOQFpn56",
        "outputId": "56120ee8-db0f-49c7-969b-fea338373b3c"
      },
      "outputs": [],
      "source": [
        "def download_and_prepare_dataset(data_info: dict):\n",
        "    \"\"\"Utility function to download the dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_info (dict): Dataset metadata.\n",
        "    \"\"\"\n",
        "    data_path = tf.keras.utils.get_file(origin=data_info[\"url\"], md5_hash=data_info[\"MD5\"])\n",
        "\n",
        "    with np.load(data_path) as data:\n",
        "        # Get videos\n",
        "        train_videos = data[\"train_images\"]\n",
        "        valid_videos = data[\"val_images\"]\n",
        "        test_videos = data[\"test_images\"]\n",
        "\n",
        "        # Get labels\n",
        "        train_labels = data[\"train_labels\"].flatten()\n",
        "        valid_labels = data[\"val_labels\"].flatten()\n",
        "        test_labels = data[\"test_labels\"].flatten()\n",
        "\n",
        "    return (\n",
        "        (train_videos, train_labels),\n",
        "        (valid_videos, valid_labels),\n",
        "        (test_videos, test_labels),\n",
        "    )\n",
        "\n",
        "\n",
        "# Get the metadata of the dataset\n",
        "info = medmnist.INFO[\"pneumoniamnist\"]\n",
        "# info = medmnist.INFO[\"retinamnist\"]\n",
        "\n",
        "\n",
        "# Get the dataset\n",
        "prepared_dataset = download_and_prepare_dataset(info)\n",
        "(x_train, y_train) = prepared_dataset[0]\n",
        "(x_val, y_val) = prepared_dataset[1]\n",
        "(x_test, y_test) = prepared_dataset[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GOpfOZc3bKF8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of train images after PCA: (4708,)\n",
            "Shape of test images after PCA: (624,)\n",
            "Shape of train labels: (4708,)\n",
            "Shape of test labels: (624,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255\n",
        "\n",
        "# Expand the dimensions of the images to (28, 28, 1) to represent the grayscale channel explicitly\n",
        "train_images = np.expand_dims(x_train, -1)\n",
        "test_images = np.expand_dims(x_test, -1)\n",
        "\n",
        "# Flatten the images to 2D arrays for PCA\n",
        "train_images_flat = train_images.reshape(train_images.shape[0], -1)  # Shape: (num_samples, 28*28)\n",
        "test_images_flat = test_images.reshape(test_images.shape[0], -1)\n",
        "\n",
        "# Initialize PCA to keep 8 components\n",
        "pca = PCA(n_components=6)\n",
        "\n",
        "# Fit PCA on the training images and transform the datasets\n",
        "train_images = pca.fit_transform(train_images_flat)\n",
        "test_images = pca.transform(test_images_flat)\n",
        "\n",
        "\n",
        "# # One-hot encode the labels\n",
        "# train_labels = to_categorical(y_train, 2)\n",
        "# test_labels = to_categorical(y_test, 2)\n",
        "# One-hot encode the labels\n",
        "train_labels = (y_train)\n",
        "test_labels = (y_test)\n",
        "# Print the shapes of the processed datasets\n",
        "print(\"Shape of train images after PCA:\", train_labels.shape)\n",
        "print(\"Shape of test images after PCA:\", test_labels.shape)\n",
        "print(\"Shape of train labels:\", train_labels.shape)\n",
        "print(\"Shape of test labels:\", test_labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXdj0ZilYdII",
        "outputId": "469ad37f-d244-40ff-b1d2-4d3c86e63bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique labels in the training set: [0 1]\n"
          ]
        }
      ],
      "source": [
        "unique_labels = np.unique(y_train)\n",
        "print(f\"Unique labels in the training set: {unique_labels}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DQ4cCjAJqaIZ"
      },
      "outputs": [],
      "source": [
        "def plot_images(images, labels, num_images=25, figsize=(10,10)):\n",
        "    grid_size = 5\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(grid_size, grid_size, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(images[i], cmap='gray')\n",
        "        plt.xlabel(f'Label: {labels[i]}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_learning_curve(history):\n",
        "    # Extracting training and validation accuracy\n",
        "    accuracy = history.history['accuracy']\n",
        "    val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "    # Extracting training and validation loss\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    # Plotting the accuracy\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(accuracy, label='Training Accuracy')\n",
        "    plt.plot(val_accuracy, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(loss, label='Training Loss')\n",
        "    plt.plot(val_loss, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Show the plots\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ryf3e3L6GKj"
      },
      "source": [
        "##  Quantum functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K7mK4jvaa_RS"
      },
      "outputs": [],
      "source": [
        "class RBSGate(Operation):\n",
        "    num_params = 1\n",
        "    num_wires = 2\n",
        "    par_domain = 'R'\n",
        "\n",
        "    def __init__(self, theta, wires):\n",
        "        super().__init__(theta, wires=wires)\n",
        "        self.theta = theta\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_matrix(theta):\n",
        "        cos = tf.cos(theta)\n",
        "        sin = tf.sin(theta)\n",
        "        return tf.convert_to_tensor([\n",
        "            [1, 0, 0, 0],\n",
        "            [0, cos, sin, 0],\n",
        "            [0, -sin, cos, 0],\n",
        "            [0, 0, 0, 1]\n",
        "        ], dtype=tf.float64)\n",
        "\n",
        "    def adjoint(self):\n",
        "        return RBSGate(-self.parameters[0], wires=self.wires)\n",
        "\n",
        "    def label(self, decimals=None, base_label=None, **kwargs):\n",
        "        theta = self.parameters[0]\n",
        "        return f\"RBS({theta:.2f})\"\n",
        "def convert_array(X):\n",
        "    alphas = tf.zeros(X.shape[:-1] + (X.shape[-1]-1,), dtype=X.dtype)\n",
        "    X_normd = tf.linalg.l2_normalize(X, axis=-1)\n",
        "    for i in range(X.shape[-1]-1):\n",
        "        prod_sin_alphas = tf.reduce_prod(tf.sin(alphas[..., :i]), axis=-1)\n",
        "        updated_value = tf.acos(X_normd[..., i] / prod_sin_alphas)\n",
        "        indices = tf.constant([[i]])\n",
        "        updates = tf.reshape(updated_value, [1])\n",
        "        alphas = tf.tensor_scatter_nd_update(alphas, indices, updates)\n",
        "    return alphas\n",
        "def vector_loader(alphas, wires=None, is_x=True, is_conjugate=False):\n",
        "    if wires is None:\n",
        "        wires = list(range(len(alphas) + 1))\n",
        "    if is_x and not is_conjugate:\n",
        "        qml.PauliX(wires=wires[0])\n",
        "    if is_conjugate:\n",
        "        for i in range(len(wires) - 2, -1, -1):\n",
        "            qml.apply(RBSGate(-alphas[i], wires=[wires[i], wires[i+1]]))\n",
        "    else:\n",
        "        for i in range(len(wires) - 1):\n",
        "            qml.apply(RBSGate(alphas[i], wires=[wires[i], wires[i+1]]))\n",
        "    if is_x and is_conjugate:\n",
        "        qml.PauliX(wires=wires[0])\n",
        "def pyramid_circuit(parameters, wires=None):\n",
        "    if wires is None:\n",
        "        length = len(qml.device.wires)\n",
        "    else:\n",
        "        length = len(wires)\n",
        "\n",
        "    k = 0\n",
        "\n",
        "    for i in range(2 * length - 2):\n",
        "        j = length - abs(length - 1 - i)\n",
        "\n",
        "        if i % 2:\n",
        "            for _ in range(j):\n",
        "                if _ % 2 == 0 and k < (parameters.shape[0]):\n",
        "                    qml.apply(RBSGate(parameters[k], wires=([wires[_], wires[_ + 1]])))\n",
        "                    k += 1\n",
        "        else:\n",
        "            for _ in range(j):\n",
        "                if _ % 2 and k < (parameters.shape[0]):\n",
        "                    qml.apply(RBSGate(parameters[k], wires=([wires[_], wires[_ + 1]])))\n",
        "                    k += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caklx0-k6RHm"
      },
      "source": [
        "# qOrthNN + Gradient Pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Define a step counter\n",
        "global_step = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
        "\n",
        "# Define a custom callback\n",
        "class StepCallback(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        global_step.assign(0)\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        global_step.assign_add(1)\n",
        "        current_step = global_step.numpy()\n",
        "        if current_step % 5 == 0:\n",
        "            # Increase the pruning ratio\n",
        "            current_prune_ratio = self.model.prune_ratio\n",
        "            # Increase prune_ratio by multiplying with exp(0.1)\n",
        "            new_prune_ratio = min(current_prune_ratio * math.exp(0.1), 1.0)  # Cap at 1.0\n",
        "            self.model.prune_ratio = new_prune_ratio\n",
        "    # def on_epoch_begin(self, epoch, logs=None):\n",
        "    #     tf.print(\"Updated prune_ratio to\", self.model.prune_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6AXTsrKB6Uui"
      },
      "outputs": [],
      "source": [
        "class HybridModel(tf.keras.Model):\n",
        "    def __init__(self,random1):\n",
        "        super(HybridModel, self).__init__()\n",
        "        \n",
        "        rd.seed(random1)\n",
        "        np.random.seed(random1)\n",
        "        tf.random.set_seed(random1)\n",
        "        qml.numpy.random.seed(random1)\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense = tf.keras.layers.Dense(6, activation='linear', dtype=tf.float64)\n",
        "        self.quantum_weights = self.add_weight(\n",
        "            shape=(15,),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "        self.dev = qml.device('default.qubit.tf', wires=6)\n",
        "        # Pruning attributes\n",
        "        self.accumulated_grads = tf.Variable(tf.zeros_like(self.quantum_weights), trainable=False)\n",
        "        self.accumulate_flag = tf.Variable(True, trainable=False)\n",
        "        \n",
        "        self.accumulate_window = tf.constant(10)\n",
        "        self.prune_window = tf.constant(8)\n",
        "        self.prune_ratio = tf.constant(0.8)\n",
        "        \n",
        "        self.accumulate_count = tf.Variable(6, dtype=tf.int32, trainable=False)\n",
        "        self.prune_count = tf.Variable(6, dtype=tf.int32, trainable=False)\n",
        "\n",
        "\n",
        "        @qml.qnode(self.dev, interface='tf', diff_method='backprop')\n",
        "        def quantum_circuit(inputs, weights):\n",
        "            inputs = tf.cast(inputs, tf.float32)\n",
        "            weights = tf.cast(weights, tf.float32)\n",
        "            vector_loader(convert_array(inputs), wires=range(6))\n",
        "            pyramid_circuit(weights, wires=range(6))\n",
        "            return [qml.expval(qml.PauliZ(wire)) for wire in range(6)]\n",
        "\n",
        "        self.quantum_circuit = quantum_circuit\n",
        "        self.classical_nn_2 = tf.keras.layers.Dense(1, activation='sigmoid', dtype=tf.float64)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = tf.cast(inputs, tf.float64)\n",
        "        flattened_inputs = self.flatten(inputs)\n",
        "        classical_output = self.dense(flattened_inputs)\n",
        "        quantum_outputs = tf.map_fn(\n",
        "            lambda x: tf.stack(self.quantum_circuit(x, self.quantum_weights)),\n",
        "            classical_output,\n",
        "            fn_output_signature=tf.TensorSpec(shape=(6,), dtype=tf.float64)\n",
        "        )\n",
        "        # Handle NaN values in quantum outputs\n",
        "        quantum_outputs = tf.where(tf.math.is_nan(quantum_outputs), tf.zeros_like(quantum_outputs), quantum_outputs)\n",
        "\n",
        "        # Combine and process quantum outputs through additional NN layers\n",
        "        quantum_outputs = tf.reshape(quantum_outputs, [-1, 6])\n",
        "        nn_output = self.classical_nn_2(quantum_outputs)\n",
        "\n",
        "        return nn_output\n",
        "    \n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        x, y = data  # Unpack the data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Compute gradients\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "\n",
        "        # Index of quantum_weights in trainable_variables\n",
        "        quantum_weights_idx = 0\n",
        "        \n",
        "        # switch flag for PGP\n",
        "        @tf.function\n",
        "        def switch_flag():\n",
        "            if self.accumulate_count == 0:\n",
        "                self.accumulate_count.assign(self.accumulate_window)\n",
        "                self.accumulate_flag.assign(False)\n",
        "            elif self.prune_count == 0:\n",
        "                self.prune_count.assign(self.prune_window)\n",
        "                self.accumulate_flag.assign(True)\n",
        "        switch_flag()\n",
        "        \n",
        "        # Probabilistic Gradient Pruning\n",
        "        if tf.equal(self.accumulate_flag, True):\n",
        "            # Step 1: Accumulate gradients for quantum_weights\n",
        "            # tf.print(\"Accu phase:\", self.accumulate_count)\n",
        "            self.accumulate_count.assign_sub(1)\n",
        "            if gradients[quantum_weights_idx] is not None:\n",
        "                self.accumulated_grads.assign_add(gradients[quantum_weights_idx])\n",
        "\n",
        "            self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        else:\n",
        "            # Step 2: Prune weights of quantum_weights\n",
        "            # tf.print(\"Pruning phase:\",self.prune_count)\n",
        "            self.prune_count.assign_sub(1)\n",
        "            # Normalize the accumulated gradients\n",
        "            grad_min = tf.reduce_min(self.accumulated_grads)\n",
        "            grad_max = tf.reduce_max(self.accumulated_grads)\n",
        "            epsilon = 1e-8\n",
        "            norm_grads = (self.accumulated_grads - grad_min) / (grad_max - grad_min + epsilon)\n",
        "\n",
        "            # Add epsilon to norm_grads to avoid log(0)\n",
        "            norm_grads_with_epsilon = norm_grads + epsilon\n",
        "\n",
        "            # Compute logits for categorical sampling\n",
        "            logits = tf.math.log(norm_grads_with_epsilon)\n",
        "\n",
        "            # Determine the number of parameters to sample\n",
        "            num_params = self.quantum_weights.shape[0]\n",
        "            num_samples = int(self.prune_ratio * num_params)\n",
        "            num_samples = tf.maximum(1, num_samples) # Ensure at least one parameter is sampled\n",
        "\n",
        "            # Sample indices based on the normalized gradients\n",
        "            indices = tf.random.categorical([logits], num_samples=num_samples)\n",
        "            indices = tf.clip_by_value(indices, 0, self.quantum_weights.shape[0] - 1)  # Ensure indices are within range\n",
        "\n",
        "            # Create a boolean mask to select the parameters to keep\n",
        "            mask = tf.zeros_like(self.quantum_weights, dtype=tf.bool)\n",
        "            indices = tf.cast(indices, tf.int32)\n",
        "            indices = tf.reshape(indices, [-1, 1])  # Ensure indices are shaped correctly\n",
        "            updates = tf.ones([tf.shape(indices)[0]], dtype=tf.bool)  # Create updates matching the indices length\n",
        "\n",
        "            mask = tf.tensor_scatter_nd_update(mask, indices, updates)\n",
        "\n",
        "            # Apply the mask to the accumulated gradients\n",
        "            pruned_grad = tf.where(mask, self.accumulated_grads[0], tf.zeros_like(self.accumulated_grads[0]))\n",
        "\n",
        "            # Apply the pruned gradient to quantum_weights\n",
        "            self.optimizer.apply_gradients([(pruned_grad, self.quantum_weights)])\n",
        "\n",
        "            # Apply gradients for other variables (excluding quantum_weights)\n",
        "            other_gradients = []\n",
        "            other_variables = []\n",
        "            for i, (grad, var) in enumerate(zip(gradients, self.trainable_variables)):\n",
        "                if i != quantum_weights_idx and grad is not None:\n",
        "                    other_gradients.append(grad)\n",
        "                    other_variables.append(var)\n",
        "            self.optimizer.apply_gradients(zip(other_gradients, other_variables))\n",
        "\n",
        "            # Reset accumulator and accumulate window\n",
        "            self.accumulated_grads.assign(tf.zeros_like(self.accumulated_grads))\n",
        "\n",
        "        # Sanitize weights: replace NaNs with zeros\n",
        "        for var in self.trainable_variables:\n",
        "            # Create a mask where NaNs are present\n",
        "            nan_mask = tf.math.is_nan(var)\n",
        "            # Replace NaNs with zeros\n",
        "            sanitized_var = tf.where(nan_mask, tf.zeros_like(var), var)\n",
        "            # Assign the sanitized variable back to the model\n",
        "            var.assign(sanitized_var)\n",
        "\n",
        "        # Update metrics\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        # tf.print(self.quantum_weights)\n",
        "        # Return a dictionary of metric results\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot the training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with random seed: 10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        }
      ],
      "source": [
        "accuracy = []\n",
        "auc = []\n",
        "for random in range(10,100,10):\n",
        "    rd.seed(random)\n",
        "    np.random.seed(random)\n",
        "    tf.random.set_seed(random)\n",
        "    qml.numpy.random.seed(random)\n",
        "    print('Training with random seed:', random)\n",
        "    model = HybridModel(random)\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',   # Metric to monitor\n",
        "        patience=3,           # Number of epochs with no improvement after which training will be stopped\n",
        "        min_delta=0.01,      # Minimum change to qualify as an improvement\n",
        "        restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored metric\n",
        "    )\n",
        "    initial_learning_rate = 0.3\n",
        "    final_learning_rate = 0.03\n",
        "\n",
        "    # Define learning rate scheduler\n",
        "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        initial_learning_rate=0.3,\n",
        "        decay_steps=16,\n",
        "        alpha=final_learning_rate / initial_learning_rate\n",
        "    )\n",
        "\n",
        "    # Adam optimizer with the cosine scheduler\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "    \n",
        "    # Train the model and store the validation accuracy\n",
        "    history = model.fit(\n",
        "        train_images, train_labels,\n",
        "        epochs=20,\n",
        "        batch_size=16,\n",
        "        validation_data=(test_images, test_labels),\n",
        "        verbose=1,\n",
        "        callbacks=[early_stopping, StepCallback()]  # Include the EarlyStopping callback\n",
        "    )\n",
        "\n",
        "    print('Testing Process')\n",
        "    test_loss, test_accuracy , test_auc = model.evaluate(test_images, test_labels)\n",
        "    accuracy.append(test_accuracy)\n",
        "    auc.append(test_auc)\n",
        "    print(f\"Test Loss: {test_loss}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy}\")\n",
        "    print(f\"Test AUC: {test_auc}\")\n",
        "\n",
        "    plot_learning_curve(history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Benchmark table \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean_acc_retina_mnist = np.mean(accuracy)\n",
        "std_acc_retina_mnist = np.std(accuracy)\n",
        "mean_auc_retina_mnist = np.mean(auc)\n",
        "std_auc_retina_mnist = np.std(auc)\n",
        "\n",
        "print(f\"Mean accuracy for Pneumonia MNIST: {mean_acc_retina_mnist:.4f} ± {std_acc_retina_mnist:.4f}\")\n",
        "print(f\"Mean AUC for Pneumonia MNIST: {mean_auc_retina_mnist:.4f} ± {std_auc_retina_mnist:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "uXpH5753pfZU"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Pennylane",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
