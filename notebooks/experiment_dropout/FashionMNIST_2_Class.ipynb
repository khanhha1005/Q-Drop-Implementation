{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc3GWn_jojsG"
      },
      "source": [
        "# Fashion MNIST notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXpH5753pfZU"
      },
      "source": [
        "## import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DG-wTTncnnjX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-22 06:35:35.522526: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-22 06:35:35.550638: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-22 06:35:35.559746: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Set TensorFlow logging to only show errors\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # This silences INFO and WARNING messages\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Add, Dense, Dropout, Embedding, GlobalAveragePooling1D, Input, Layer, LayerNormalization, MultiHeadAttention\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "try:\n",
        "  import pennylane as qml\n",
        "except:\n",
        "  !pip install pennylane\n",
        "  import pennylane as qml\n",
        "from pennylane.operation import Operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oa2gttKppTU"
      },
      "source": [
        "## Import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set random seed\n",
        "random = 10 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP2uhOQFpn56",
        "outputId": "56120ee8-db0f-49c7-969b-fea338373b3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of train labels: (500, 2)\n",
            "Shape of test labels: (300, 2)\n"
          ]
        }
      ],
      "source": [
        "# Expand the dimensions of the images to (28, 28, 1) to represent the grayscale channel explicitly\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Filter for only the classes 0 and 3\n",
        "train_filter = np.where((y_train == 0) | (y_train == 1))\n",
        "test_filter = np.where((y_test == 0) | (y_test == 1))\n",
        "\n",
        "x_train, y_train = x_train[train_filter], y_train[train_filter]\n",
        "x_test, y_test = x_test[test_filter], y_test[test_filter]\n",
        "\n",
        "# Use the first 500 images as the training set\n",
        "x_train, y_train = x_train[:500], y_train[:500]\n",
        "# Randomly sample 300 images from the remaining data as the validation set\n",
        "x_val, y_val = x_test[500:800], y_test[500:800]\n",
        "\n",
        "x_test, y_test = x_test[:300], y_test[:300]\n",
        "\n",
        "# Rescale the images\n",
        "x_train = x_train / 255.0\n",
        "x_val = x_val / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "train_images = np.expand_dims(x_train, -1)\n",
        "test_images = np.expand_dims(x_test, -1)\n",
        "\n",
        "def preprocess_images(images):\n",
        "    # Center-crop to 24x24\n",
        "    images_cropped = tf.image.central_crop(images, central_fraction=24/28)\n",
        "    \n",
        "    # Down-sample to 4x4\n",
        "    images_downsampled = tf.image.resize(images_cropped, size=(4, 4), method=tf.image.ResizeMethod.BILINEAR)\n",
        "    \n",
        "    return images_downsampled\n",
        "\n",
        "# Preprocess the train, validation, and test images\n",
        "train_images = preprocess_images(train_images)\n",
        "test_images= preprocess_images(test_images)\n",
        "\n",
        "# Map the labels 3 -> 0 and 6 -> 1\n",
        "y_train_binary = np.where(y_train == 3, 0, 1)\n",
        "y_test_binary = np.where(y_test == 3, 0, 1)\n",
        "\n",
        "# Convert the labels to one-hot encoded vectors\n",
        "train_labels = to_categorical(y_train_binary, 2)\n",
        "test_labels = to_categorical(y_test_binary, 2)\n",
        "\n",
        "print(\"Shape of train labels:\", train_labels.shape)\n",
        "print(\"Shape of test labels:\", test_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXdj0ZilYdII",
        "outputId": "469ad37f-d244-40ff-b1d2-4d3c86e63bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique labels in the training set: [0 3]\n"
          ]
        }
      ],
      "source": [
        "unique_labels = np.unique(y_train)\n",
        "print(f\"Unique labels in the training set: {unique_labels}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DQ4cCjAJqaIZ"
      },
      "outputs": [],
      "source": [
        "def plot_images(images, labels, num_images=25, figsize=(10,10)):\n",
        "    grid_size = 5\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(grid_size, grid_size, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(images[i], cmap='gray')\n",
        "        plt.xlabel(f'Label: {labels[i]}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ryf3e3L6GKj"
      },
      "source": [
        "##  Quantum functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "K7mK4jvaa_RS"
      },
      "outputs": [],
      "source": [
        "class RBSGate(Operation):\n",
        "    num_params = 1\n",
        "    num_wires = 2\n",
        "    par_domain = 'R'\n",
        "\n",
        "    def __init__(self, theta, wires):\n",
        "        super().__init__(theta, wires=wires)\n",
        "        self.theta = theta\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_matrix(theta):\n",
        "        cos = tf.cos(theta)\n",
        "        sin = tf.sin(theta)\n",
        "        return tf.convert_to_tensor([\n",
        "            [1, 0, 0, 0],\n",
        "            [0, cos, sin, 0],\n",
        "            [0, -sin, cos, 0],\n",
        "            [0, 0, 0, 1]\n",
        "        ], dtype=tf.float64)\n",
        "\n",
        "    def adjoint(self):\n",
        "        return RBSGate(-self.parameters[0], wires=self.wires)\n",
        "\n",
        "    def label(self, decimals=None, base_label=None, **kwargs):\n",
        "        theta = self.parameters[0]\n",
        "        return f\"RBS({theta:.2f})\"\n",
        "def convert_array(X):\n",
        "    alphas = tf.zeros(X.shape[:-1] + (X.shape[-1]-1,), dtype=X.dtype)\n",
        "    X_normd = tf.linalg.l2_normalize(X, axis=-1)\n",
        "    for i in range(X.shape[-1]-1):\n",
        "        prod_sin_alphas = tf.reduce_prod(tf.sin(alphas[..., :i]), axis=-1)\n",
        "        updated_value = tf.acos(X_normd[..., i] / prod_sin_alphas)\n",
        "        indices = tf.constant([[i]])\n",
        "        updates = tf.reshape(updated_value, [1])\n",
        "        alphas = tf.tensor_scatter_nd_update(alphas, indices, updates)\n",
        "    return alphas\n",
        "def vector_loader(alphas, wires=None, is_x=True, is_conjugate=False):\n",
        "    if wires is None:\n",
        "        wires = list(range(len(alphas) + 1))\n",
        "    if is_x and not is_conjugate:\n",
        "        qml.PauliX(wires=wires[0])\n",
        "    if is_conjugate:\n",
        "        for i in range(len(wires) - 2, -1, -1):\n",
        "            qml.apply(RBSGate(-alphas[i], wires=[wires[i], wires[i+1]]))\n",
        "    else:\n",
        "        for i in range(len(wires) - 1):\n",
        "            qml.apply(RBSGate(alphas[i], wires=[wires[i], wires[i+1]]))\n",
        "    if is_x and is_conjugate:\n",
        "        qml.PauliX(wires=wires[0])\n",
        "def pyramid_circuit(parameters, wires=None):\n",
        "    if wires is None:\n",
        "        length = len(qml.device.wires)\n",
        "    else:\n",
        "        length = len(wires)\n",
        "\n",
        "    k = 0\n",
        "\n",
        "    for i in range(2 * length - 2):\n",
        "        j = length - abs(length - 1 - i)\n",
        "\n",
        "        if i % 2:\n",
        "            for _ in range(j):\n",
        "                if _ % 2 == 0 and k < (parameters.shape[0]):\n",
        "                    qml.apply(RBSGate(parameters[k], wires=([wires[_], wires[_ + 1]])))\n",
        "                    k += 1\n",
        "        else:\n",
        "            for _ in range(j):\n",
        "                if _ % 2 and k < (parameters.shape[0]):\n",
        "                    qml.apply(RBSGate(parameters[k], wires=([wires[_], wires[_ + 1]])))\n",
        "                    k += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caklx0-k6RHm"
      },
      "source": [
        "# qOrthNN + Dynamic Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6AXTsrKB6Uui"
      },
      "outputs": [],
      "source": [
        "class HybridModel(tf.keras.Model):\n",
        "    def __init__(self,apply_quantum_dropout):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense = tf.keras.layers.Dense(6, activation='linear', dtype=tf.float64)\n",
        "        self.quantum_weights2 = self.add_weight(\n",
        "            shape=(15,),\n",
        "            initializer='zeros',\n",
        "            trainable=True,\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "        self.theta_locked = self.add_weight(\n",
        "            shape=(15,),\n",
        "            initializer='zeros',\n",
        "            trainable=False,\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "        \n",
        "        self.dev = qml.device('default.qubit.tf', wires=6)\n",
        "\n",
        "        @qml.qnode(self.dev, interface='tf', diff_method='backprop')\n",
        "        def quantum_circuit(inputs, weights):\n",
        "            inputs = tf.cast(inputs, tf.float32)\n",
        "            weights = tf.cast(weights, tf.float32)\n",
        "            vector_loader(convert_array(inputs), wires=range(6))\n",
        "            pyramid_circuit(weights, wires=range(6))\n",
        "            return [qml.expval(qml.PauliZ(wire)) for wire in range(6)]\n",
        "\n",
        "        self.quantum_circuit = quantum_circuit\n",
        "        self.classical_nn_2 = tf.keras.layers.Dense(2, activation='sigmoid', dtype=tf.float64)\n",
        "        \n",
        "        # Droppout mask\n",
        "        self.theta_wire_0 = tf.constant([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1], dtype=tf.int32)\n",
        "        self.theta_wire_1 = tf.constant([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0], dtype=tf.int32)\n",
        "        # Only Drop 1 Qubit or 1 Wire \n",
        "        self.n_drop = tf.constant(1, dtype=tf.int32)\n",
        "            \n",
        "        self.drop_flag = tf.Variable(apply_quantum_dropout, trainable=False)\n",
        "        \n",
        "        \n",
        "    def call(self, inputs):\n",
        "        inputs = tf.cast(inputs, tf.float64)\n",
        "        flattened_inputs = self.flatten(inputs)\n",
        "        classical_output = self.dense(flattened_inputs)\n",
        "        quantum_outputs = tf.map_fn(\n",
        "            lambda x: tf.stack(self.quantum_circuit(x, self.quantum_weights2)),\n",
        "            classical_output,\n",
        "            fn_output_signature=tf.TensorSpec(shape=(6,), dtype=tf.float64)\n",
        "        )\n",
        "\n",
        "        # Apply the condition using tf.cond\n",
        "        quantum_outputs = tf.cond(\n",
        "            self.drop_flag,\n",
        "            lambda: tf.concat([\n",
        "                tf.zeros((tf.shape(quantum_outputs)[0], 1), dtype=tf.float64),  # Create a tensor of zeros\n",
        "                quantum_outputs[:, 1:]  # Keep the rest of the elements\n",
        "            ], axis=1),\n",
        "            lambda: quantum_outputs  # Keep original quantum_outputs if drop_flag is False\n",
        "        )\n",
        "        # Handle NaN values in quantum outputs\n",
        "        quantum_outputs = tf.where(tf.math.is_nan(quantum_outputs), tf.zeros_like(quantum_outputs), quantum_outputs)\n",
        "\n",
        "        # Combine and process quantum outputs through additional NN layers\n",
        "        quantum_outputs = tf.reshape(quantum_outputs, [-1, 6])\n",
        "        nn_output = self.classical_nn_2(quantum_outputs)\n",
        "\n",
        "        return nn_output\n",
        "    def train_step(self, data):\n",
        "        x, y = data  # Unpack the data\n",
        "\n",
        "        # Lock the dropped gates\n",
        "        self.theta_locked = tf.identity(self.quantum_weights2)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Compute gradients\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "\n",
        "        # Sanitize gradients: replace NaNs with zeros\n",
        "        sanitized_gradients = []\n",
        "        \n",
        "        for grad in gradients:\n",
        "            if grad is not None:\n",
        "                # Replace NaNs with zeros\n",
        "                grad = tf.where(tf.math.is_nan(grad), tf.zeros_like(grad), grad)\n",
        "            sanitized_gradients.append(grad)\n",
        "           \n",
        "        # Apply the dropout mask: set the elements to 0 where mask is 1 \n",
        "        def one_wire_drop():\n",
        "            quantum_masked_gradients = tf.where(self.theta_wire_0 == 1, 0.0, gradients[0])\n",
        "            updated_gradients = sanitized_gradients\n",
        "            updated_gradients[0] = quantum_masked_gradients\n",
        "            return updated_gradients\n",
        "        def two_wire_drop():\n",
        "            quantum_masked_gradients = tf.where(self.theta_wire_0 == 1, 0.0, gradients[0])\n",
        "            quantum_masked_gradients = tf.where(self.theta_wire_1 == 1, 0.0, quantum_masked_gradients)\n",
        "            updated_gradients = sanitized_gradients\n",
        "            updated_gradients[0] = quantum_masked_gradients\n",
        "            return updated_gradients\n",
        "        def no_wire_drop():\n",
        "            sanitized_gradients1 = []\n",
        "            for grad in gradients:\n",
        "                if grad is not None:\n",
        "                    # Replace NaNs with zeros\n",
        "                    grad = tf.where(tf.math.is_nan(grad), tf.zeros_like(grad), grad)\n",
        "                sanitized_gradients1.append(grad)\n",
        "            return sanitized_gradients1\n",
        "        # Apply tf.cond based on the value of self.n_drop\n",
        "        sanitized_gradients = tf.cond(\n",
        "            tf.logical_and(tf.equal(self.n_drop, 1), tf.equal(self.drop_flag, True)),  # Combine conditions\n",
        "            one_wire_drop,  # If both conditions are true, execute one_wire_drop\n",
        "            no_wire_drop    # If either condition is false, execute no_wire_drop\n",
        "        )\n",
        "        # Apply the sanitized gradients\n",
        "        self.optimizer.apply_gradients(zip(sanitized_gradients, self.trainable_variables))\n",
        "            \n",
        "        # tf.print(\"Drop:\", self.drop_flag)\n",
        "        # tf.print(self.theta_locked, summarize=-1)\n",
        "        # tf.print(self.quantum_weights2, summarize=-1)\n",
        "        #make weights that are nan 0\n",
        "        for var in self.trainable_variables:\n",
        "            # Create a mask where NaNs are present\n",
        "            nan_mask = tf.math.is_nan(var)\n",
        "            # Replace NaNs with zeros\n",
        "            sanitized_var = tf.where(nan_mask, tf.zeros_like(var), var)\n",
        "            # Assign the sanitized variable back to the model\n",
        "            var.assign(sanitized_var)\n",
        "        # Update metrics\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        # tf.print(self.quantum_weights2, summarize=-1)\n",
        "\n",
        "        # Return a dictionary of metric results\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a custom callback to log and update the learning rate\n",
        "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.new_lr = None\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Get the current learning rate\n",
        "        current_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
        "        # Store the learning rate for the next iteration\n",
        "        self.new_lr = current_lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Monter Carlo Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 117ms/step - accuracy: 0.5275 - loss: 0.5112 - val_accuracy: 0.8733 - val_loss: 0.6140\n",
            "Epochs 2\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 112ms/step - accuracy: 0.8452 - loss: 0.4858 - val_accuracy: 0.8733 - val_loss: 0.3562\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 115ms/step - accuracy: 0.8217 - loss: 0.3870 - val_accuracy: 0.8667 - val_loss: 0.3604\n",
            "Epochs 4\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 118ms/step - accuracy: 0.7310 - loss: 0.5378 - val_accuracy: 0.8633 - val_loss: 0.3666\n",
            "Epochs 5\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 126ms/step - accuracy: 0.6694 - loss: 0.4227 - val_accuracy: 0.8900 - val_loss: 0.3955\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 121ms/step - accuracy: 0.5277 - loss: 0.5473 - val_accuracy: 0.8633 - val_loss: 0.6465\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 117ms/step - accuracy: 0.7245 - loss: 0.5211 - val_accuracy: 0.8700 - val_loss: 0.4034\n",
            "Epochs 8\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 120ms/step - accuracy: 0.5314 - loss: 0.5247 - val_accuracy: 0.8467 - val_loss: 0.5435\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 125ms/step - accuracy: 0.6802 - loss: 0.4729 - val_accuracy: 0.8500 - val_loss: 0.4418\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 118ms/step - accuracy: 0.5275 - loss: 0.5112 - val_accuracy: 0.8733 - val_loss: 0.6140\n",
            "Epochs 2\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 115ms/step - accuracy: 0.8452 - loss: 0.4858 - val_accuracy: 0.8733 - val_loss: 0.3562\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 115ms/step - accuracy: 0.8217 - loss: 0.3870 - val_accuracy: 0.8667 - val_loss: 0.3604\n",
            "Epochs 4\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 120ms/step - accuracy: 0.7310 - loss: 0.5378 - val_accuracy: 0.8633 - val_loss: 0.3666\n",
            "Epochs 5\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 125ms/step - accuracy: 0.6694 - loss: 0.4227 - val_accuracy: 0.8900 - val_loss: 0.3955\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 117ms/step - accuracy: 0.5277 - loss: 0.5473 - val_accuracy: 0.8633 - val_loss: 0.6465\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 114ms/step - accuracy: 0.7245 - loss: 0.5211 - val_accuracy: 0.8700 - val_loss: 0.4034\n",
            "Epochs 8\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 129ms/step - accuracy: 0.5314 - loss: 0.5247 - val_accuracy: 0.8467 - val_loss: 0.5435\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 120ms/step - accuracy: 0.6802 - loss: 0.4729 - val_accuracy: 0.8500 - val_loss: 0.4418\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 116ms/step - accuracy: 0.5275 - loss: 0.5112 - val_accuracy: 0.8733 - val_loss: 0.6140\n",
            "Epochs 2\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 114ms/step - accuracy: 0.8452 - loss: 0.4858 - val_accuracy: 0.8733 - val_loss: 0.3562\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 120ms/step - accuracy: 0.8217 - loss: 0.3870 - val_accuracy: 0.8667 - val_loss: 0.3604\n",
            "Epochs 4\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 114ms/step - accuracy: 0.7310 - loss: 0.5378 - val_accuracy: 0.8633 - val_loss: 0.3666\n",
            "Epochs 5\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 114ms/step - accuracy: 0.6694 - loss: 0.4227 - val_accuracy: 0.8900 - val_loss: 0.3955\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 118ms/step - accuracy: 0.5277 - loss: 0.5473 - val_accuracy: 0.8633 - val_loss: 0.6465\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 113ms/step - accuracy: 0.7245 - loss: 0.5211 - val_accuracy: 0.8700 - val_loss: 0.4034\n",
            "Epochs 8\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 114ms/step - accuracy: 0.5314 - loss: 0.5247 - val_accuracy: 0.8467 - val_loss: 0.5435\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 114ms/step - accuracy: 0.6802 - loss: 0.4729 - val_accuracy: 0.8500 - val_loss: 0.4418\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 116ms/step - accuracy: 0.5275 - loss: 0.5112 - val_accuracy: 0.8733 - val_loss: 0.6140\n",
            "Epochs 2\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 117ms/step - accuracy: 0.8452 - loss: 0.4858 - val_accuracy: 0.8733 - val_loss: 0.3562\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 116ms/step - accuracy: 0.8217 - loss: 0.3870 - val_accuracy: 0.8667 - val_loss: 0.3604\n",
            "Epochs 4\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 120ms/step - accuracy: 0.7310 - loss: 0.5378 - val_accuracy: 0.8633 - val_loss: 0.3666\n",
            "Epochs 5\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 132ms/step - accuracy: 0.6694 - loss: 0.4227 - val_accuracy: 0.8900 - val_loss: 0.3955\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 110ms/step - accuracy: 0.5277 - loss: 0.5473 - val_accuracy: 0.8633 - val_loss: 0.6465\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 113ms/step - accuracy: 0.7245 - loss: 0.5211 - val_accuracy: 0.8700 - val_loss: 0.4034\n",
            "Epochs 8\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 133ms/step - accuracy: 0.5314 - loss: 0.5247 - val_accuracy: 0.8467 - val_loss: 0.5435\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 120ms/step - accuracy: 0.6802 - loss: 0.4729 - val_accuracy: 0.8500 - val_loss: 0.4418\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 116ms/step - accuracy: 0.5275 - loss: 0.5112 - val_accuracy: 0.8733 - val_loss: 0.6140\n",
            "Epochs 2\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 115ms/step - accuracy: 0.8452 - loss: 0.4858 - val_accuracy: 0.8733 - val_loss: 0.3562\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 118ms/step - accuracy: 0.8217 - loss: 0.3870 - val_accuracy: 0.8667 - val_loss: 0.3604\n",
            "Epochs 4\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 120ms/step - accuracy: 0.7310 - loss: 0.5378 - val_accuracy: 0.8633 - val_loss: 0.3666\n",
            "Epochs 5\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 109ms/step - accuracy: 0.6694 - loss: 0.4227 - val_accuracy: 0.8900 - val_loss: 0.3955\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.5277 - loss: 0.5473 - val_accuracy: 0.8633 - val_loss: 0.6465\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 116ms/step - accuracy: 0.7245 - loss: 0.5211 - val_accuracy: 0.8700 - val_loss: 0.4034\n",
            "Epochs 8\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 118ms/step - accuracy: 0.5314 - loss: 0.5247 - val_accuracy: 0.8467 - val_loss: 0.5435\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 126ms/step - accuracy: 0.6802 - loss: 0.4729 - val_accuracy: 0.8500 - val_loss: 0.4418\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 117ms/step - accuracy: 0.5275 - loss: 0.5112 - val_accuracy: 0.8733 - val_loss: 0.6140\n",
            "Epochs 2\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.8452 - loss: 0.4858 - val_accuracy: 0.8733 - val_loss: 0.3562\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 121ms/step - accuracy: 0.8217 - loss: 0.3870 - val_accuracy: 0.8667 - val_loss: 0.3604\n",
            "Epochs 4\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 118ms/step - accuracy: 0.7310 - loss: 0.5378 - val_accuracy: 0.8633 - val_loss: 0.3666\n",
            "Epochs 5\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 120ms/step - accuracy: 0.6694 - loss: 0.4227 - val_accuracy: 0.8900 - val_loss: 0.3955\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 130ms/step - accuracy: 0.5277 - loss: 0.5473 - val_accuracy: 0.8633 - val_loss: 0.6465\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 119ms/step - accuracy: 0.7245 - loss: 0.5211 - val_accuracy: 0.8700 - val_loss: 0.4034\n",
            "Epochs 8\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 122ms/step - accuracy: 0.5314 - loss: 0.5247 - val_accuracy: 0.8467 - val_loss: 0.5435\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 120ms/step - accuracy: 0.6802 - loss: 0.4729 - val_accuracy: 0.8500 - val_loss: 0.4418\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 123ms/step - accuracy: 0.5275 - loss: 0.5112 - val_accuracy: 0.8733 - val_loss: 0.6140\n",
            "Epochs 2\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 121ms/step - accuracy: 0.8452 - loss: 0.4858 - val_accuracy: 0.8733 - val_loss: 0.3562\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 126ms/step - accuracy: 0.8217 - loss: 0.3870 - val_accuracy: 0.8667 - val_loss: 0.3604\n",
            "Epochs 4\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 116ms/step - accuracy: 0.7310 - loss: 0.5378 - val_accuracy: 0.8633 - val_loss: 0.3666\n",
            "Epochs 5\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 119ms/step - accuracy: 0.6694 - loss: 0.4227 - val_accuracy: 0.8900 - val_loss: 0.3955\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 120ms/step - accuracy: 0.5277 - loss: 0.5473 - val_accuracy: 0.8633 - val_loss: 0.6465\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 123ms/step - accuracy: 0.7245 - loss: 0.5211 - val_accuracy: 0.8700 - val_loss: 0.4034\n",
            "Epochs 8\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 128ms/step - accuracy: 0.5314 - loss: 0.5247 - val_accuracy: 0.8467 - val_loss: 0.5435\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 125ms/step - accuracy: 0.6802 - loss: 0.4729 - val_accuracy: 0.8500 - val_loss: 0.4418\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 122ms/step - accuracy: 0.5275 - loss: 0.5112 - val_accuracy: 0.8733 - val_loss: 0.6140\n",
            "Epochs 2\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 115ms/step - accuracy: 0.8452 - loss: 0.4858 - val_accuracy: 0.8733 - val_loss: 0.3562\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 123ms/step - accuracy: 0.8217 - loss: 0.3870 - val_accuracy: 0.8667 - val_loss: 0.3604\n",
            "Epochs 4\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 121ms/step - accuracy: 0.7310 - loss: 0.5378 - val_accuracy: 0.8633 - val_loss: 0.3666\n",
            "Epochs 5\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 129ms/step - accuracy: 0.6694 - loss: 0.4227 - val_accuracy: 0.8900 - val_loss: 0.3955\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 117ms/step - accuracy: 0.5277 - loss: 0.5473 - val_accuracy: 0.8633 - val_loss: 0.6465\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 117ms/step - accuracy: 0.7245 - loss: 0.5211 - val_accuracy: 0.8700 - val_loss: 0.4034\n",
            "Epochs 8\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 134ms/step - accuracy: 0.5314 - loss: 0.5247 - val_accuracy: 0.8467 - val_loss: 0.5435\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 130ms/step - accuracy: 0.6802 - loss: 0.4729 - val_accuracy: 0.8500 - val_loss: 0.4418\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 121ms/step - accuracy: 0.5275 - loss: 0.5112 - val_accuracy: 0.8733 - val_loss: 0.6140\n",
            "Epochs 2\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 118ms/step - accuracy: 0.8452 - loss: 0.4858 - val_accuracy: 0.8733 - val_loss: 0.3562\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 118ms/step - accuracy: 0.8217 - loss: 0.3870 - val_accuracy: 0.8667 - val_loss: 0.3604\n",
            "Epochs 4\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 119ms/step - accuracy: 0.7310 - loss: 0.5378 - val_accuracy: 0.8633 - val_loss: 0.3666\n",
            "Epochs 5\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 113ms/step - accuracy: 0.6694 - loss: 0.4227 - val_accuracy: 0.8900 - val_loss: 0.3955\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 112ms/step - accuracy: 0.5277 - loss: 0.5473 - val_accuracy: 0.8633 - val_loss: 0.6465\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 115ms/step - accuracy: 0.7245 - loss: 0.5211 - val_accuracy: 0.8700 - val_loss: 0.4034\n",
            "Epochs 8\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 120ms/step - accuracy: 0.5314 - loss: 0.5247 - val_accuracy: 0.8467 - val_loss: 0.5435\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 120ms/step - accuracy: 0.6802 - loss: 0.4729 - val_accuracy: 0.8500 - val_loss: 0.4418\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 118ms/step - accuracy: 0.5275 - loss: 0.5112 - val_accuracy: 0.8733 - val_loss: 0.6140\n",
            "Epochs 2\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 119ms/step - accuracy: 0.8452 - loss: 0.4858 - val_accuracy: 0.8733 - val_loss: 0.3562\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 116ms/step - accuracy: 0.8217 - loss: 0.3870 - val_accuracy: 0.8667 - val_loss: 0.3604\n",
            "Epochs 4\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 109ms/step - accuracy: 0.7310 - loss: 0.5378 - val_accuracy: 0.8633 - val_loss: 0.3666\n",
            "Epochs 5\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 113ms/step - accuracy: 0.6694 - loss: 0.4227 - val_accuracy: 0.8900 - val_loss: 0.3955\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 112ms/step - accuracy: 0.5277 - loss: 0.5473 - val_accuracy: 0.8633 - val_loss: 0.6465\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 122ms/step - accuracy: 0.7245 - loss: 0.5211 - val_accuracy: 0.8700 - val_loss: 0.4034\n",
            "Epochs 8\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.5314 - loss: 0.5247 - val_accuracy: 0.8467 - val_loss: 0.5435\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 119ms/step - accuracy: 0.6802 - loss: 0.4729 - val_accuracy: 0.8500 - val_loss: 0.4418\n"
          ]
        }
      ],
      "source": [
        "accuracy = []\n",
        "import random as rd\n",
        "auc = []\n",
        "\n",
        "# Lists to store metrics for plotting\n",
        "all_losses = []  # For training loss\n",
        "all_val_losses = []  # For validation loss\n",
        "all_train_acc = []  # For training accuracy\n",
        "all_val_acc = []  # For validation accuracy\n",
        "seeds = []\n",
        "# Number of Monte Carlo simulations\n",
        "n_simulations = 10\n",
        "\n",
        "# Container for results\n",
        "accuracies = []\n",
        "aucs = []\n",
        "n_simulations = 10 \n",
        "# Monte Carlo Resampling\n",
        "for _ in range(n_simulations):\n",
        "    if os.path.exists('/home/HardDisk/quang_nguyen/quantum_ml/experiment_dropout/quantum_weights2.weights.h5'):\n",
        "        os.remove('/home/HardDisk/quang_nguyen/quantum_ml/experiment_dropout/quantum_weights2.weights.h5')\n",
        "    rd.seed(random)\n",
        "    np.random.seed(random)\n",
        "    tf.random.set_seed(random)\n",
        "    qml.numpy.random.seed(random)\n",
        "    print('Training with random seed:', random)\n",
        "    initial_lr = 0.3\n",
        "    seed_losses = []\n",
        "    seed_val_losses = []\n",
        "    seed_train_acc = []\n",
        "    seed_val_acc = []\n",
        "    for iteration in range(1, 10):\n",
        "\n",
        "        print(f\"Epochs {iteration}\")\n",
        "\n",
        "        # Switch dropout flag randomly\n",
        "        if rd.random() <= 0.7 and iteration != 1 :\n",
        "            drop_flag = True\n",
        "            print(\"Applying Quantum Dropout\")\n",
        "        else:\n",
        "            drop_flag = False\n",
        "            print(\"Not Applying Quantum Dropout\")\n",
        "        # Create the model\n",
        "        model = HybridModel(apply_quantum_dropout=drop_flag)\n",
        "        initial_learning_rate = initial_lr\n",
        "        final_learning_rate = 0.03\n",
        "        # Define learning rate scheduler\n",
        "        lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "            initial_learning_rate=initial_lr,\n",
        "            decay_steps=16,\n",
        "            alpha=final_learning_rate / initial_learning_rate\n",
        "        )\n",
        "\n",
        "        # Adam optimizer with the cosine scheduler\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        lr_logger = LearningRateLogger()\n",
        "\n",
        "        # Train the model with the callback\n",
        "        history = model.fit(\n",
        "            train_images, train_labels,\n",
        "            epochs=1,  # Train for 1 epoch at a time in this loop\n",
        "            batch_size=20,\n",
        "            verbose=1,\n",
        "            validation_data=(test_images, test_labels),\n",
        "            callbacks=[lr_logger]  # Use the learning rate logger\n",
        "        )\n",
        "\n",
        "        # Update the learning rate for the next iteration\n",
        "        if lr_logger.new_lr:\n",
        "            initial_lr = lr_logger.new_lr\n",
        "\n",
        "        # Store losses for this iteration\n",
        "        seed_losses.append(history.history['loss'][0])  # Training loss\n",
        "        seed_val_losses.append(history.history['val_loss'][0])  # Validation loss\n",
        "        seed_train_acc.append(history.history['accuracy'][0])  # Training accuracy\n",
        "        seed_val_acc.append(history.history['val_accuracy'][0])  # Validation accuracy\n",
        "        # Save the weights after training\n",
        "        model.save_weights('/home/HardDisk/quang_nguyen/quantum_ml/experiment_dropout/quantum_weights2.weights.h5')\n",
        "    # Store the metrics for the seed\n",
        "    seeds.append(random)\n",
        "    all_losses.append(seed_losses)\n",
        "    all_val_losses.append(seed_val_losses)\n",
        "    all_train_acc.append(seed_train_acc)\n",
        "    all_val_acc.append(seed_val_acc)\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated Accuracy: 0.8900\n",
            "95% Confidence Interval for Accuracy: [0.8900, 0.8900]\n"
          ]
        }
      ],
      "source": [
        "    \n",
        "accuracies = [max(sublist) for sublist in all_val_acc]\n",
        "\n",
        "mean_accuracy = np.mean(accuracies)\n",
        "ci_accuracy_lower = np.percentile(accuracies, 2.5)\n",
        "ci_accuracy_upper = np.percentile(accuracies, 97.5)\n",
        "\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(f\"Estimated Accuracy: {mean_accuracy:.4f}\")\n",
        "print(f\"95% Confidence Interval for Accuracy: [{ci_accuracy_lower:.4f}, {ci_accuracy_upper:.4f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 100 Epochs Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 188ms/step - accuracy: 0.4733 - loss: 0.5373 - val_accuracy: 0.5233 - val_loss: 0.6652\n",
            "Epochs 2\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 197ms/step - accuracy: 0.8602 - loss: 0.4790 - val_accuracy: 0.8767 - val_loss: 0.3821\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 189ms/step - accuracy: 0.8054 - loss: 0.3483 - val_accuracy: 0.8567 - val_loss: 0.4041\n",
            "Epochs 4\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 193ms/step - accuracy: 0.7314 - loss: 0.5509 - val_accuracy: 0.8567 - val_loss: 0.4039\n",
            "Epochs 5\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 185ms/step - accuracy: 0.6042 - loss: 0.4115 - val_accuracy: 0.8567 - val_loss: 0.4751\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 180ms/step - accuracy: 0.5632 - loss: 0.5689 - val_accuracy: 0.8467 - val_loss: 0.6420\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 177ms/step - accuracy: 0.6742 - loss: 0.5307 - val_accuracy: 0.8700 - val_loss: 0.4405\n",
            "Epochs 8\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 176ms/step - accuracy: 0.5576 - loss: 0.5396 - val_accuracy: 0.8500 - val_loss: 0.5149\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 191ms/step - accuracy: 0.5687 - loss: 0.4615 - val_accuracy: 0.8400 - val_loss: 0.5564\n",
            "Epochs 10\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 182ms/step - accuracy: 0.4857 - loss: 0.5252 - val_accuracy: 0.8567 - val_loss: 0.6527\n",
            "Epochs 11\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 183ms/step - accuracy: 0.5068 - loss: 0.6542 - val_accuracy: 0.8633 - val_loss: 0.5293\n",
            "Epochs 12\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 186ms/step - accuracy: 0.6148 - loss: 0.5080 - val_accuracy: 0.8767 - val_loss: 0.4620\n",
            "Epochs 13\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 176ms/step - accuracy: 0.6157 - loss: 0.6116 - val_accuracy: 0.8533 - val_loss: 0.5047\n",
            "Epochs 14\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 183ms/step - accuracy: 0.5750 - loss: 0.5737 - val_accuracy: 0.8833 - val_loss: 0.5878\n",
            "Epochs 15\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 179ms/step - accuracy: 0.6957 - loss: 0.5607 - val_accuracy: 0.8600 - val_loss: 0.4304\n",
            "Epochs 16\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 194ms/step - accuracy: 0.6066 - loss: 0.5458 - val_accuracy: 0.6200 - val_loss: 0.6442\n",
            "Epochs 17\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 185ms/step - accuracy: 0.5548 - loss: 0.4951 - val_accuracy: 0.8467 - val_loss: 0.5969\n",
            "Epochs 18\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 201ms/step - accuracy: 0.6657 - loss: 0.5251 - val_accuracy: 0.8600 - val_loss: 0.4658\n",
            "Epochs 19\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 191ms/step - accuracy: 0.6122 - loss: 0.5619 - val_accuracy: 0.8633 - val_loss: 0.5927\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "x and y must have same first dimension, but have shapes (99,) and (19,)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 94\u001b[0m\n\u001b[1;32m     92\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, losses \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_losses):\n\u001b[0;32m---> 94\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSeed \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m (Train Loss)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, val_losses \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_val_losses):\n\u001b[1;32m     96\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(epochs, val_losses, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseeds[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Test Loss)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/Pennylane/lib/python3.11/site-packages/matplotlib/pyplot.py:3794\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3786\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3788\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3793\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/Pennylane/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
            "File \u001b[0;32m~/anaconda3/envs/Pennylane/lib/python3.11/site-packages/matplotlib/axes/_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/Pennylane/lib/python3.11/site-packages/matplotlib/axes/_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (99,) and (19,)"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "accuracy = []\n",
        "import random as rd\n",
        "auc = []\n",
        "import os\n",
        "# Lists to store metrics for plotting\n",
        "all_losses = []  # For training loss\n",
        "all_val_losses = []  # For validation loss\n",
        "all_train_acc = []  # For training accuracy\n",
        "all_val_acc = []  # For validation accuracy\n",
        "seeds = []\n",
        "for random in [10]:\n",
        "    if os.path.exists('/home/HardDisk/quang_nguyen/quantum_ml/experiment_dropout/quantum_weights2.weights.h5'):\n",
        "        os.remove('/home/HardDisk/quang_nguyen/quantum_ml/experiment_dropout/quantum_weights2.weights.h5')\n",
        "    rd.seed(random)\n",
        "    np.random.seed(random)\n",
        "    tf.random.set_seed(random)\n",
        "    qml.numpy.random.seed(random)\n",
        "    print('Training with random seed:', random)\n",
        "    initial_lr = 0.3\n",
        "    seed_losses = []\n",
        "    seed_val_losses = []\n",
        "    seed_train_acc = []\n",
        "    seed_val_acc = []\n",
        "    for iteration in range(1, 20):\n",
        "\n",
        "        print(f\"Epochs {iteration}\")\n",
        "\n",
        "        # Switch dropout flag randomly\n",
        "        if rd.random() <= 0.7 and iteration != 1 :\n",
        "            drop_flag = True\n",
        "            print(\"Applying Quantum Dropout\")\n",
        "        else:\n",
        "            drop_flag = False\n",
        "            print(\"Not Applying Quantum Dropout\")\n",
        "        # Create the model\n",
        "        model = HybridModel(apply_quantum_dropout=drop_flag)\n",
        "        initial_learning_rate = initial_lr\n",
        "        final_learning_rate = 0.03\n",
        "        # Define learning rate scheduler\n",
        "        lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "            initial_learning_rate=initial_lr,\n",
        "            decay_steps=16,\n",
        "            alpha=final_learning_rate / initial_learning_rate\n",
        "        )\n",
        "\n",
        "        # Adam optimizer with the cosine scheduler\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        lr_logger = LearningRateLogger()\n",
        "\n",
        "        # Train the model with the callback\n",
        "        history = model.fit(\n",
        "            train_images, train_labels,\n",
        "            epochs=1,  # Train for 1 epoch at a time in this loop\n",
        "            batch_size=32,\n",
        "            verbose=1,\n",
        "            validation_data=(test_images, test_labels),\n",
        "            callbacks=[lr_logger]  # Use the learning rate logger\n",
        "        )\n",
        "\n",
        "        # Update the learning rate for the next iteration\n",
        "        if lr_logger.new_lr:\n",
        "            initial_lr = lr_logger.new_lr\n",
        "\n",
        "        # Store losses for this iteration\n",
        "        seed_losses.append(history.history['loss'][0])  # Training loss\n",
        "        seed_val_losses.append(history.history['val_loss'][0])  # Validation loss\n",
        "        seed_train_acc.append(history.history['accuracy'][0])  # Training accuracy\n",
        "        seed_val_acc.append(history.history['val_accuracy'][0])  # Validation accuracy\n",
        "        # Save the weights after training\n",
        "        model.save_weights('/home/HardDisk/quang_nguyen/quantum_ml/experiment_dropout/quantum_weights2.weights.h5')\n",
        "    # Store the metrics for the seed\n",
        "    seeds.append(random)\n",
        "    all_losses.append(seed_losses)\n",
        "    all_val_losses.append(seed_val_losses)\n",
        "    all_train_acc.append(seed_train_acc)\n",
        "    all_val_acc.append(seed_val_acc)\n",
        "\n",
        "    # Store the losses for the seed\n",
        "    # Plotting losses and accuracies for each seed\n",
        "    epochs = range(1, 100)  # Number of epochs\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(2, 2, 1)\n",
        "    for i, losses in enumerate(all_losses):\n",
        "        plt.plot(epochs, losses, label=f'Seed {seeds[i]} (Train Loss)')\n",
        "    for i, val_losses in enumerate(all_val_losses):\n",
        "        plt.plot(epochs, val_losses, linestyle='--', label=f'Seed {seeds[i]} (Test Loss)')\n",
        "    plt.title('Loss per Epoch for Each Seed')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    # Training accuracy plot\n",
        "    plt.subplot(2, 2, 2)\n",
        "    for i, train_acc in enumerate(all_train_acc):\n",
        "        plt.plot(epochs, train_acc, label=f'Seed {seeds[i]} (Train Acc)')\n",
        "    for i, val_acc in enumerate(all_val_acc):\n",
        "        plt.plot(epochs, val_acc, linestyle='--', label=f'Seed {seeds[i]} (Test Acc)')\n",
        "    plt.title('Accuracy per Epoch for Each Seed')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "uXpH5753pfZU"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Pennylane",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
