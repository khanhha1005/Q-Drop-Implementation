{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc3GWn_jojsG"
      },
      "source": [
        "# Retina MNIST notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXpH5753pfZU"
      },
      "source": [
        "## import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DG-wTTncnnjX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Set TensorFlow logging to only show errors\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # This silences INFO and WARNING messages\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Add, Dense, Dropout, Embedding, GlobalAveragePooling1D, Input, Layer, LayerNormalization, MultiHeadAttention\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "try:\n",
        "  import pennylane as qml\n",
        "except:\n",
        "  !pip install pennylane\n",
        "  import pennylane as qml\n",
        "from pennylane.operation import Operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oa2gttKppTU"
      },
      "source": [
        "## Import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Vr_vMdH-FlP",
        "outputId": "3238e9ed-36db-40d5-a7b8-c5cc0917030c"
      },
      "outputs": [],
      "source": [
        "# Set the random seed\n",
        "random = 10 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "t9VleEId-FW2"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import medmnist\n",
        "from medmnist import INFO, Evaluator\n",
        "import random as rd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP2uhOQFpn56",
        "outputId": "56120ee8-db0f-49c7-969b-fea338373b3c"
      },
      "outputs": [],
      "source": [
        "def download_and_prepare_dataset(data_info: dict):\n",
        "    \"\"\"Utility function to download the dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_info (dict): Dataset metadata.\n",
        "    \"\"\"\n",
        "    data_path = tf.keras.utils.get_file(origin=data_info[\"url\"], md5_hash=data_info[\"MD5\"])\n",
        "\n",
        "    with np.load(data_path) as data:\n",
        "        # Get videos\n",
        "        train_videos = data[\"train_images\"]\n",
        "        valid_videos = data[\"val_images\"]\n",
        "        test_videos = data[\"test_images\"]\n",
        "\n",
        "        # Get labels\n",
        "        train_labels = data[\"train_labels\"].flatten()\n",
        "        valid_labels = data[\"val_labels\"].flatten()\n",
        "        test_labels = data[\"test_labels\"].flatten()\n",
        "\n",
        "    return (\n",
        "        (train_videos, train_labels),\n",
        "        (valid_videos, valid_labels),\n",
        "        (test_videos, test_labels),\n",
        "    )\n",
        "\n",
        "\n",
        "# Get the metadata of the dataset\n",
        "info = medmnist.INFO[\"retinamnist\"]\n",
        "# info = medmnist.INFO[\"retinamnist\"]\n",
        "\n",
        "\n",
        "# Get the dataset\n",
        "prepared_dataset = download_and_prepare_dataset(info)\n",
        "(x_train, y_train) = prepared_dataset[0]\n",
        "(x_val, y_val) = prepared_dataset[1]\n",
        "(x_test, y_test) = prepared_dataset[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GOpfOZc3bKF8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of train images after PCA: (1080, 2)\n",
            "Shape of test images after PCA: (400, 2)\n",
            "Shape of train labels: (1080, 2)\n",
            "Shape of test labels: (400, 2)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255\n",
        "\n",
        "# Expand the dimensions of the images to (28, 28, 1) to represent the grayscale channel explicitly\n",
        "train_images = np.expand_dims(x_train, -1)\n",
        "test_images = np.expand_dims(x_test, -1)\n",
        "\n",
        "# Flatten the images to 2D arrays for PCA\n",
        "train_images_flat = train_images.reshape(train_images.shape[0], -1)  # Shape: (num_samples, 28*28)\n",
        "test_images_flat = test_images.reshape(test_images.shape[0], -1)\n",
        "\n",
        "# Initialize PCA to keep 8 components\n",
        "pca = PCA(n_components=6)\n",
        "\n",
        "# Fit PCA on the training images and transform the datasets\n",
        "train_images = pca.fit_transform(train_images_flat)\n",
        "test_images = pca.transform(test_images_flat)\n",
        "\n",
        "# Map the labels 3 -> 0 and 6 -> 1\n",
        "y_train = np.where(y_train == 0, 0, 1)\n",
        "y_test = np.where(y_test == 0, 0, 1)\n",
        "\n",
        "# One-hot encode the labels\n",
        "train_labels = to_categorical(y_train, 2)\n",
        "test_labels = to_categorical(y_test, 2)\n",
        "\n",
        "# Print the shapes of the processed datasets\n",
        "print(\"Shape of train images after PCA:\", train_labels.shape)\n",
        "print(\"Shape of test images after PCA:\", test_labels.shape)\n",
        "print(\"Shape of train labels:\", train_labels.shape)\n",
        "print(\"Shape of test labels:\", test_labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXdj0ZilYdII",
        "outputId": "469ad37f-d244-40ff-b1d2-4d3c86e63bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique labels in the training set: [0 1]\n"
          ]
        }
      ],
      "source": [
        "unique_labels = np.unique(y_train)\n",
        "print(f\"Unique labels in the training set: {unique_labels}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "DQ4cCjAJqaIZ"
      },
      "outputs": [],
      "source": [
        "def plot_images(images, labels, num_images=25, figsize=(10,10)):\n",
        "    grid_size = 5\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(grid_size, grid_size, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(images[i], cmap='gray')\n",
        "        plt.xlabel(f'Label: {labels[i]}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_learning_curve(history):\n",
        "    # Extracting training and validation accuracy\n",
        "    accuracy = history.history['accuracy']\n",
        "    val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "    # Extracting training and validation loss\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    # Plotting the accuracy\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(accuracy, label='Training Accuracy')\n",
        "    plt.plot(val_accuracy, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(loss, label='Training Loss')\n",
        "    plt.plot(val_loss, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Show the plots\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ryf3e3L6GKj"
      },
      "source": [
        "##  Quantum functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "K7mK4jvaa_RS"
      },
      "outputs": [],
      "source": [
        "class RBSGate(Operation):\n",
        "    num_params = 1\n",
        "    num_wires = 2\n",
        "    par_domain = 'R'\n",
        "\n",
        "    def __init__(self, theta, wires):\n",
        "        super().__init__(theta, wires=wires)\n",
        "        self.theta = theta\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_matrix(theta):\n",
        "        cos = tf.cos(theta)\n",
        "        sin = tf.sin(theta)\n",
        "        return tf.convert_to_tensor([\n",
        "            [1, 0, 0, 0],\n",
        "            [0, cos, sin, 0],\n",
        "            [0, -sin, cos, 0],\n",
        "            [0, 0, 0, 1]\n",
        "        ], dtype=tf.float64)\n",
        "\n",
        "    def adjoint(self):\n",
        "        return RBSGate(-self.parameters[0], wires=self.wires)\n",
        "\n",
        "    def label(self, decimals=None, base_label=None, **kwargs):\n",
        "        theta = self.parameters[0]\n",
        "        return f\"RBS({theta:.2f})\"\n",
        "def convert_array(X):\n",
        "    alphas = tf.zeros(X.shape[:-1] + (X.shape[-1]-1,), dtype=X.dtype)\n",
        "    X_normd = tf.linalg.l2_normalize(X, axis=-1)\n",
        "    for i in range(X.shape[-1]-1):\n",
        "        prod_sin_alphas = tf.reduce_prod(tf.sin(alphas[..., :i]), axis=-1)\n",
        "        updated_value = tf.acos(X_normd[..., i] / prod_sin_alphas)\n",
        "        indices = tf.constant([[i]])\n",
        "        updates = tf.reshape(updated_value, [1])\n",
        "        alphas = tf.tensor_scatter_nd_update(alphas, indices, updates)\n",
        "    return alphas\n",
        "def vector_loader(alphas, wires=None, is_x=True, is_conjugate=False):\n",
        "    if wires is None:\n",
        "        wires = list(range(len(alphas) + 1))\n",
        "    if is_x and not is_conjugate:\n",
        "        qml.PauliX(wires=wires[0])\n",
        "    if is_conjugate:\n",
        "        for i in range(len(wires) - 2, -1, -1):\n",
        "            qml.apply(RBSGate(-alphas[i], wires=[wires[i], wires[i+1]]))\n",
        "    else:\n",
        "        for i in range(len(wires) - 1):\n",
        "            qml.apply(RBSGate(alphas[i], wires=[wires[i], wires[i+1]]))\n",
        "    if is_x and is_conjugate:\n",
        "        qml.PauliX(wires=wires[0])\n",
        "def pyramid_circuit(parameters, wires=None):\n",
        "    if wires is None:\n",
        "        length = len(qml.device.wires)\n",
        "    else:\n",
        "        length = len(wires)\n",
        "\n",
        "    k = 0\n",
        "\n",
        "    for i in range(2 * length - 2):\n",
        "        j = length - abs(length - 1 - i)\n",
        "\n",
        "        if i % 2:\n",
        "            for _ in range(j):\n",
        "                if _ % 2 == 0 and k < (parameters.shape[0]):\n",
        "                    qml.apply(RBSGate(parameters[k], wires=([wires[_], wires[_ + 1]])))\n",
        "                    k += 1\n",
        "        else:\n",
        "            for _ in range(j):\n",
        "                if _ % 2 and k < (parameters.shape[0]):\n",
        "                    qml.apply(RBSGate(parameters[k], wires=([wires[_], wires[_ + 1]])))\n",
        "                    k += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caklx0-k6RHm"
      },
      "source": [
        "# qOrthNN + Dynamic Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6AXTsrKB6Uui"
      },
      "outputs": [],
      "source": [
        "class HybridModel(tf.keras.Model):\n",
        "    def __init__(self,apply_quantum_dropout):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense = tf.keras.layers.Dense(6, activation='linear', dtype=tf.float64)\n",
        "        self.quantum_weights = self.add_weight(\n",
        "            shape=(15,),\n",
        "            initializer='zeros',\n",
        "            trainable=True,\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "        self.theta_locked = self.add_weight(\n",
        "            shape=(15,),\n",
        "            initializer='zeros',\n",
        "            trainable=False,\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "        \n",
        "        self.dev = qml.device('default.qubit.tf', wires=6)\n",
        "\n",
        "        @qml.qnode(self.dev, interface='tf', diff_method='backprop')\n",
        "        def quantum_circuit(inputs, weights):\n",
        "            inputs = tf.cast(inputs, tf.float32)\n",
        "            weights = tf.cast(weights, tf.float32)\n",
        "            vector_loader(convert_array(inputs), wires=range(6))\n",
        "            pyramid_circuit(weights, wires=range(6))\n",
        "            return [qml.expval(qml.PauliZ(wire)) for wire in range(6)]\n",
        "\n",
        "        self.quantum_circuit = quantum_circuit\n",
        "        self.classical_nn_2 = tf.keras.layers.Dense(2, activation='sigmoid', dtype=tf.float64)\n",
        "        \n",
        "        # Droppout mask\n",
        "        self.theta_wire_0 = tf.constant([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1], dtype=tf.int32)\n",
        "        self.theta_wire_1 = tf.constant([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0], dtype=tf.int32)\n",
        "        # Only Drop 1 Qubit or 1 Wire \n",
        "        self.n_drop = tf.constant(1, dtype=tf.int32)\n",
        "            \n",
        "        self.drop_flag = tf.Variable(apply_quantum_dropout, trainable=False)\n",
        "        \n",
        "        \n",
        "    def call(self, inputs):\n",
        "        inputs = tf.cast(inputs, tf.float64)\n",
        "        flattened_inputs = self.flatten(inputs)\n",
        "        classical_output = self.dense(flattened_inputs)\n",
        "        quantum_outputs = tf.map_fn(\n",
        "            lambda x: tf.stack(self.quantum_circuit(x, self.quantum_weights)),\n",
        "            classical_output,\n",
        "            fn_output_signature=tf.TensorSpec(shape=(6,), dtype=tf.float64)\n",
        "        )\n",
        "\n",
        "        # Apply the condition using tf.cond\n",
        "        quantum_outputs = tf.cond(\n",
        "            self.drop_flag,\n",
        "            lambda: tf.concat([\n",
        "                tf.zeros((tf.shape(quantum_outputs)[0], 1), dtype=tf.float64),  # Create a tensor of zeros\n",
        "                quantum_outputs[:, 1:]  # Keep the rest of the elements\n",
        "            ], axis=1),\n",
        "            lambda: quantum_outputs  # Keep original quantum_outputs if drop_flag is False\n",
        "        )\n",
        "        # Handle NaN values in quantum outputs\n",
        "        quantum_outputs = tf.where(tf.math.is_nan(quantum_outputs), tf.zeros_like(quantum_outputs), quantum_outputs)\n",
        "\n",
        "        # Combine and process quantum outputs through additional NN layers\n",
        "        quantum_outputs = tf.reshape(quantum_outputs, [-1, 6])\n",
        "        nn_output = self.classical_nn_2(quantum_outputs)\n",
        "\n",
        "        return nn_output\n",
        "    def train_step(self, data):\n",
        "        x, y = data  # Unpack the data\n",
        "\n",
        "        # Lock the dropped gates\n",
        "        self.theta_locked = tf.identity(self.quantum_weights)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Compute gradients\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "\n",
        "        # Sanitize gradients: replace NaNs with zeros\n",
        "        sanitized_gradients = []\n",
        "        \n",
        "        for grad in gradients:\n",
        "            if grad is not None:\n",
        "                # Replace NaNs with zeros\n",
        "                grad = tf.where(tf.math.is_nan(grad), tf.zeros_like(grad), grad)\n",
        "            sanitized_gradients.append(grad)\n",
        "           \n",
        "        # Apply the dropout mask: set the elements to 0 where mask is 1 \n",
        "        def one_wire_drop():\n",
        "            quantum_masked_gradients = tf.where(self.theta_wire_0 == 1, 0.0, gradients[0])\n",
        "            updated_gradients = sanitized_gradients\n",
        "            updated_gradients[0] = quantum_masked_gradients\n",
        "            return updated_gradients\n",
        "        def two_wire_drop():\n",
        "            quantum_masked_gradients = tf.where(self.theta_wire_0 == 1, 0.0, gradients[0])\n",
        "            quantum_masked_gradients = tf.where(self.theta_wire_1 == 1, 0.0, quantum_masked_gradients)\n",
        "            updated_gradients = sanitized_gradients\n",
        "            updated_gradients[0] = quantum_masked_gradients\n",
        "            return updated_gradients\n",
        "        def no_wire_drop():\n",
        "            sanitized_gradients1 = []\n",
        "            for grad in gradients:\n",
        "                if grad is not None:\n",
        "                    # Replace NaNs with zeros\n",
        "                    grad = tf.where(tf.math.is_nan(grad), tf.zeros_like(grad), grad)\n",
        "                sanitized_gradients1.append(grad)\n",
        "            return sanitized_gradients1\n",
        "        # Apply tf.cond based on the value of self.n_drop\n",
        "        sanitized_gradients = tf.cond(\n",
        "            tf.logical_and(tf.equal(self.n_drop, 1), tf.equal(self.drop_flag, True)),  # Combine conditions\n",
        "            one_wire_drop,  # If both conditions are true, execute one_wire_drop\n",
        "            no_wire_drop    # If either condition is false, execute no_wire_drop\n",
        "        )\n",
        "        # Apply the sanitized gradients\n",
        "        self.optimizer.apply_gradients(zip(sanitized_gradients, self.trainable_variables))\n",
        "            \n",
        "        # tf.print(\"Drop:\", self.drop_flag)\n",
        "        # tf.print(self.theta_locked, summarize=-1)\n",
        "        # tf.print(self.quantum_weights, summarize=-1)\n",
        "        #make weights that are nan 0\n",
        "        for var in self.trainable_variables:\n",
        "            # Create a mask where NaNs are present\n",
        "            nan_mask = tf.math.is_nan(var)\n",
        "            # Replace NaNs with zeros\n",
        "            sanitized_var = tf.where(nan_mask, tf.zeros_like(var), var)\n",
        "            # Assign the sanitized variable back to the model\n",
        "            var.assign(sanitized_var)\n",
        "        # Update metrics\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        # tf.print(self.quantum_weights, summarize=-1)\n",
        "\n",
        "        # Return a dictionary of metric results\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a custom callback to log and update the learning rate\n",
        "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.new_lr = None\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Get the current learning rate\n",
        "        current_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
        "        # Store the learning rate for the next iteration\n",
        "        self.new_lr = current_lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Monte Carlo Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 137ms/step - AUC: 0.5369 - accuracy: 0.5336 - loss: 0.5103 - val_AUC: 0.6792 - val_accuracy: 0.6250 - val_loss: 0.6406\n",
            "Epochs 2\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 140ms/step - AUC: 0.6320 - accuracy: 0.6308 - loss: 0.4824 - val_AUC: 0.7274 - val_accuracy: 0.6675 - val_loss: 0.6161\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 120ms/step - AUC: 0.5801 - accuracy: 0.5746 - loss: 0.3983 - val_AUC: 0.6081 - val_accuracy: 0.6100 - val_loss: 0.6914\n",
            "Epochs 4\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 128ms/step - AUC: 0.6057 - accuracy: 0.5988 - loss: 0.5292 - val_AUC: 0.6419 - val_accuracy: 0.6300 - val_loss: 0.6656\n",
            "Epochs 5\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 135ms/step - AUC: 0.6140 - accuracy: 0.6216 - loss: 0.4358 - val_AUC: 0.7660 - val_accuracy: 0.7175 - val_loss: 0.5765\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 116ms/step - AUC: 0.5209 - accuracy: 0.5058 - loss: 0.5493 - val_AUC: 0.7138 - val_accuracy: 0.6900 - val_loss: 0.6449\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 118ms/step - AUC: 0.6113 - accuracy: 0.6035 - loss: 0.5126 - val_AUC: 0.6831 - val_accuracy: 0.6475 - val_loss: 0.6425\n",
            "Epochs 8\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 130ms/step - AUC: 0.6310 - accuracy: 0.5994 - loss: 0.5289 - val_AUC: 0.4489 - val_accuracy: 0.4725 - val_loss: 0.7328\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 119ms/step - AUC: 0.4767 - accuracy: 0.4790 - loss: 0.4876 - val_AUC: 0.5761 - val_accuracy: 0.5675 - val_loss: 0.6830\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 131ms/step - AUC: 0.5369 - accuracy: 0.5336 - loss: 0.5103 - val_AUC: 0.6792 - val_accuracy: 0.6250 - val_loss: 0.6406\n",
            "Epochs 2\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 122ms/step - AUC: 0.6320 - accuracy: 0.6308 - loss: 0.4824 - val_AUC: 0.7274 - val_accuracy: 0.6675 - val_loss: 0.6161\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 135ms/step - AUC: 0.5801 - accuracy: 0.5746 - loss: 0.3983 - val_AUC: 0.6081 - val_accuracy: 0.6100 - val_loss: 0.6914\n",
            "Epochs 4\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 128ms/step - AUC: 0.6057 - accuracy: 0.5988 - loss: 0.5292 - val_AUC: 0.6419 - val_accuracy: 0.6300 - val_loss: 0.6656\n",
            "Epochs 5\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 124ms/step - AUC: 0.6140 - accuracy: 0.6216 - loss: 0.4358 - val_AUC: 0.7660 - val_accuracy: 0.7175 - val_loss: 0.5765\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 139ms/step - AUC: 0.5209 - accuracy: 0.5058 - loss: 0.5493 - val_AUC: 0.7138 - val_accuracy: 0.6900 - val_loss: 0.6449\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 145ms/step - AUC: 0.6113 - accuracy: 0.6035 - loss: 0.5126 - val_AUC: 0.6831 - val_accuracy: 0.6475 - val_loss: 0.6425\n",
            "Epochs 8\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - AUC: 0.6310 - accuracy: 0.5994 - loss: 0.5289 - val_AUC: 0.4489 - val_accuracy: 0.4725 - val_loss: 0.7328\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 122ms/step - AUC: 0.4767 - accuracy: 0.4790 - loss: 0.4876 - val_AUC: 0.5761 - val_accuracy: 0.5675 - val_loss: 0.6830\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 143ms/step - AUC: 0.5369 - accuracy: 0.5336 - loss: 0.5103 - val_AUC: 0.6792 - val_accuracy: 0.6250 - val_loss: 0.6406\n",
            "Epochs 2\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 144ms/step - AUC: 0.6320 - accuracy: 0.6308 - loss: 0.4824 - val_AUC: 0.7274 - val_accuracy: 0.6675 - val_loss: 0.6161\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - AUC: 0.5801 - accuracy: 0.5746 - loss: 0.3983 - val_AUC: 0.6081 - val_accuracy: 0.6100 - val_loss: 0.6914\n",
            "Epochs 4\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 125ms/step - AUC: 0.6057 - accuracy: 0.5988 - loss: 0.5292 - val_AUC: 0.6419 - val_accuracy: 0.6300 - val_loss: 0.6656\n",
            "Epochs 5\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 128ms/step - AUC: 0.6140 - accuracy: 0.6216 - loss: 0.4358 - val_AUC: 0.7660 - val_accuracy: 0.7175 - val_loss: 0.5765\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 126ms/step - AUC: 0.5209 - accuracy: 0.5058 - loss: 0.5493 - val_AUC: 0.7138 - val_accuracy: 0.6900 - val_loss: 0.6449\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 131ms/step - AUC: 0.6113 - accuracy: 0.6035 - loss: 0.5126 - val_AUC: 0.6831 - val_accuracy: 0.6475 - val_loss: 0.6425\n",
            "Epochs 8\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 130ms/step - AUC: 0.6310 - accuracy: 0.5994 - loss: 0.5289 - val_AUC: 0.4489 - val_accuracy: 0.4725 - val_loss: 0.7328\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 128ms/step - AUC: 0.4767 - accuracy: 0.4790 - loss: 0.4876 - val_AUC: 0.5761 - val_accuracy: 0.5675 - val_loss: 0.6830\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 127ms/step - AUC: 0.5369 - accuracy: 0.5336 - loss: 0.5103 - val_AUC: 0.6792 - val_accuracy: 0.6250 - val_loss: 0.6406\n",
            "Epochs 2\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 121ms/step - AUC: 0.6320 - accuracy: 0.6308 - loss: 0.4824 - val_AUC: 0.7274 - val_accuracy: 0.6675 - val_loss: 0.6161\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 119ms/step - AUC: 0.5801 - accuracy: 0.5746 - loss: 0.3983 - val_AUC: 0.6081 - val_accuracy: 0.6100 - val_loss: 0.6914\n",
            "Epochs 4\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 144ms/step - AUC: 0.6057 - accuracy: 0.5988 - loss: 0.5292 - val_AUC: 0.6419 - val_accuracy: 0.6300 - val_loss: 0.6656\n",
            "Epochs 5\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 145ms/step - AUC: 0.6140 - accuracy: 0.6216 - loss: 0.4358 - val_AUC: 0.7660 - val_accuracy: 0.7175 - val_loss: 0.5765\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 127ms/step - AUC: 0.5209 - accuracy: 0.5058 - loss: 0.5493 - val_AUC: 0.7138 - val_accuracy: 0.6900 - val_loss: 0.6449\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 122ms/step - AUC: 0.6113 - accuracy: 0.6035 - loss: 0.5126 - val_AUC: 0.6831 - val_accuracy: 0.6475 - val_loss: 0.6425\n",
            "Epochs 8\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 132ms/step - AUC: 0.6310 - accuracy: 0.5994 - loss: 0.5289 - val_AUC: 0.4489 - val_accuracy: 0.4725 - val_loss: 0.7328\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 119ms/step - AUC: 0.4767 - accuracy: 0.4790 - loss: 0.4876 - val_AUC: 0.5761 - val_accuracy: 0.5675 - val_loss: 0.6830\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 130ms/step - AUC: 0.5369 - accuracy: 0.5336 - loss: 0.5103 - val_AUC: 0.6792 - val_accuracy: 0.6250 - val_loss: 0.6406\n",
            "Epochs 2\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 128ms/step - AUC: 0.6320 - accuracy: 0.6308 - loss: 0.4824 - val_AUC: 0.7274 - val_accuracy: 0.6675 - val_loss: 0.6161\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 123ms/step - AUC: 0.5801 - accuracy: 0.5746 - loss: 0.3983 - val_AUC: 0.6081 - val_accuracy: 0.6100 - val_loss: 0.6914\n",
            "Epochs 4\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 122ms/step - AUC: 0.6057 - accuracy: 0.5988 - loss: 0.5292 - val_AUC: 0.6419 - val_accuracy: 0.6300 - val_loss: 0.6656\n",
            "Epochs 5\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 120ms/step - AUC: 0.6140 - accuracy: 0.6216 - loss: 0.4358 - val_AUC: 0.7660 - val_accuracy: 0.7175 - val_loss: 0.5765\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 125ms/step - AUC: 0.5209 - accuracy: 0.5058 - loss: 0.5493 - val_AUC: 0.7138 - val_accuracy: 0.6900 - val_loss: 0.6449\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 137ms/step - AUC: 0.6113 - accuracy: 0.6035 - loss: 0.5126 - val_AUC: 0.6831 - val_accuracy: 0.6475 - val_loss: 0.6425\n",
            "Epochs 8\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 126ms/step - AUC: 0.6310 - accuracy: 0.5994 - loss: 0.5289 - val_AUC: 0.4489 - val_accuracy: 0.4725 - val_loss: 0.7328\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 121ms/step - AUC: 0.4767 - accuracy: 0.4790 - loss: 0.4876 - val_AUC: 0.5761 - val_accuracy: 0.5675 - val_loss: 0.6830\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 133ms/step - AUC: 0.5369 - accuracy: 0.5336 - loss: 0.5103 - val_AUC: 0.6792 - val_accuracy: 0.6250 - val_loss: 0.6406\n",
            "Epochs 2\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 122ms/step - AUC: 0.6320 - accuracy: 0.6308 - loss: 0.4824 - val_AUC: 0.7274 - val_accuracy: 0.6675 - val_loss: 0.6161\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 122ms/step - AUC: 0.5801 - accuracy: 0.5746 - loss: 0.3983 - val_AUC: 0.6081 - val_accuracy: 0.6100 - val_loss: 0.6914\n",
            "Epochs 4\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 132ms/step - AUC: 0.6057 - accuracy: 0.5988 - loss: 0.5292 - val_AUC: 0.6419 - val_accuracy: 0.6300 - val_loss: 0.6656\n",
            "Epochs 5\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 128ms/step - AUC: 0.6140 - accuracy: 0.6216 - loss: 0.4358 - val_AUC: 0.7660 - val_accuracy: 0.7175 - val_loss: 0.5765\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 123ms/step - AUC: 0.5209 - accuracy: 0.5058 - loss: 0.5493 - val_AUC: 0.7138 - val_accuracy: 0.6900 - val_loss: 0.6449\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 115ms/step - AUC: 0.6113 - accuracy: 0.6035 - loss: 0.5126 - val_AUC: 0.6831 - val_accuracy: 0.6475 - val_loss: 0.6425\n",
            "Epochs 8\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 128ms/step - AUC: 0.6310 - accuracy: 0.5994 - loss: 0.5289 - val_AUC: 0.4489 - val_accuracy: 0.4725 - val_loss: 0.7328\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 129ms/step - AUC: 0.4767 - accuracy: 0.4790 - loss: 0.4876 - val_AUC: 0.5761 - val_accuracy: 0.5675 - val_loss: 0.6830\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 118ms/step - AUC: 0.5369 - accuracy: 0.5336 - loss: 0.5103 - val_AUC: 0.6792 - val_accuracy: 0.6250 - val_loss: 0.6406\n",
            "Epochs 2\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 119ms/step - AUC: 0.6320 - accuracy: 0.6308 - loss: 0.4824 - val_AUC: 0.7274 - val_accuracy: 0.6675 - val_loss: 0.6161\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 136ms/step - AUC: 0.5801 - accuracy: 0.5746 - loss: 0.3983 - val_AUC: 0.6081 - val_accuracy: 0.6100 - val_loss: 0.6914\n",
            "Epochs 4\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 126ms/step - AUC: 0.6057 - accuracy: 0.5988 - loss: 0.5292 - val_AUC: 0.6419 - val_accuracy: 0.6300 - val_loss: 0.6656\n",
            "Epochs 5\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 317ms/step - AUC: 0.6140 - accuracy: 0.6216 - loss: 0.4358 - val_AUC: 0.7660 - val_accuracy: 0.7175 - val_loss: 0.5765\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 120ms/step - AUC: 0.5209 - accuracy: 0.5058 - loss: 0.5493 - val_AUC: 0.7138 - val_accuracy: 0.6900 - val_loss: 0.6449\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 127ms/step - AUC: 0.6113 - accuracy: 0.6035 - loss: 0.5126 - val_AUC: 0.6831 - val_accuracy: 0.6475 - val_loss: 0.6425\n",
            "Epochs 8\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 131ms/step - AUC: 0.6310 - accuracy: 0.5994 - loss: 0.5289 - val_AUC: 0.4489 - val_accuracy: 0.4725 - val_loss: 0.7328\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 133ms/step - AUC: 0.4767 - accuracy: 0.4790 - loss: 0.4876 - val_AUC: 0.5761 - val_accuracy: 0.5675 - val_loss: 0.6830\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 126ms/step - AUC: 0.5369 - accuracy: 0.5336 - loss: 0.5103 - val_AUC: 0.6792 - val_accuracy: 0.6250 - val_loss: 0.6406\n",
            "Epochs 2\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 125ms/step - AUC: 0.6320 - accuracy: 0.6308 - loss: 0.4824 - val_AUC: 0.7274 - val_accuracy: 0.6675 - val_loss: 0.6161\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 118ms/step - AUC: 0.5801 - accuracy: 0.5746 - loss: 0.3983 - val_AUC: 0.6081 - val_accuracy: 0.6100 - val_loss: 0.6914\n",
            "Epochs 4\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 123ms/step - AUC: 0.6057 - accuracy: 0.5988 - loss: 0.5292 - val_AUC: 0.6419 - val_accuracy: 0.6300 - val_loss: 0.6656\n",
            "Epochs 5\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 130ms/step - AUC: 0.6140 - accuracy: 0.6216 - loss: 0.4358 - val_AUC: 0.7660 - val_accuracy: 0.7175 - val_loss: 0.5765\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 124ms/step - AUC: 0.5209 - accuracy: 0.5058 - loss: 0.5493 - val_AUC: 0.7138 - val_accuracy: 0.6900 - val_loss: 0.6449\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 124ms/step - AUC: 0.6113 - accuracy: 0.6035 - loss: 0.5126 - val_AUC: 0.6831 - val_accuracy: 0.6475 - val_loss: 0.6425\n",
            "Epochs 8\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 123ms/step - AUC: 0.6310 - accuracy: 0.5994 - loss: 0.5289 - val_AUC: 0.4489 - val_accuracy: 0.4725 - val_loss: 0.7328\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 132ms/step - AUC: 0.4767 - accuracy: 0.4790 - loss: 0.4876 - val_AUC: 0.5761 - val_accuracy: 0.5675 - val_loss: 0.6830\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 133ms/step - AUC: 0.5369 - accuracy: 0.5336 - loss: 0.5103 - val_AUC: 0.6792 - val_accuracy: 0.6250 - val_loss: 0.6406\n",
            "Epochs 2\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 133ms/step - AUC: 0.6320 - accuracy: 0.6308 - loss: 0.4824 - val_AUC: 0.7274 - val_accuracy: 0.6675 - val_loss: 0.6161\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 124ms/step - AUC: 0.5801 - accuracy: 0.5746 - loss: 0.3983 - val_AUC: 0.6081 - val_accuracy: 0.6100 - val_loss: 0.6914\n",
            "Epochs 4\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 129ms/step - AUC: 0.6057 - accuracy: 0.5988 - loss: 0.5292 - val_AUC: 0.6419 - val_accuracy: 0.6300 - val_loss: 0.6656\n",
            "Epochs 5\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 130ms/step - AUC: 0.6140 - accuracy: 0.6216 - loss: 0.4358 - val_AUC: 0.7660 - val_accuracy: 0.7175 - val_loss: 0.5765\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 117ms/step - AUC: 0.5209 - accuracy: 0.5058 - loss: 0.5493 - val_AUC: 0.7138 - val_accuracy: 0.6900 - val_loss: 0.6449\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 125ms/step - AUC: 0.6113 - accuracy: 0.6035 - loss: 0.5126 - val_AUC: 0.6831 - val_accuracy: 0.6475 - val_loss: 0.6425\n",
            "Epochs 8\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 124ms/step - AUC: 0.6310 - accuracy: 0.5994 - loss: 0.5289 - val_AUC: 0.4489 - val_accuracy: 0.4725 - val_loss: 0.7328\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 125ms/step - AUC: 0.4767 - accuracy: 0.4790 - loss: 0.4876 - val_AUC: 0.5761 - val_accuracy: 0.5675 - val_loss: 0.6830\n",
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 130ms/step - AUC: 0.5369 - accuracy: 0.5336 - loss: 0.5103 - val_AUC: 0.6792 - val_accuracy: 0.6250 - val_loss: 0.6406\n",
            "Epochs 2\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 120ms/step - AUC: 0.6320 - accuracy: 0.6308 - loss: 0.4824 - val_AUC: 0.7274 - val_accuracy: 0.6675 - val_loss: 0.6161\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 121ms/step - AUC: 0.5801 - accuracy: 0.5746 - loss: 0.3983 - val_AUC: 0.6081 - val_accuracy: 0.6100 - val_loss: 0.6914\n",
            "Epochs 4\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 130ms/step - AUC: 0.6057 - accuracy: 0.5988 - loss: 0.5292 - val_AUC: 0.6419 - val_accuracy: 0.6300 - val_loss: 0.6656\n",
            "Epochs 5\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 137ms/step - AUC: 0.6140 - accuracy: 0.6216 - loss: 0.4358 - val_AUC: 0.7660 - val_accuracy: 0.7175 - val_loss: 0.5765\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 136ms/step - AUC: 0.5209 - accuracy: 0.5058 - loss: 0.5493 - val_AUC: 0.7138 - val_accuracy: 0.6900 - val_loss: 0.6449\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 122ms/step - AUC: 0.6113 - accuracy: 0.6035 - loss: 0.5126 - val_AUC: 0.6831 - val_accuracy: 0.6475 - val_loss: 0.6425\n",
            "Epochs 8\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 125ms/step - AUC: 0.6310 - accuracy: 0.5994 - loss: 0.5289 - val_AUC: 0.4489 - val_accuracy: 0.4725 - val_loss: 0.7328\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 125ms/step - AUC: 0.4767 - accuracy: 0.4790 - loss: 0.4876 - val_AUC: 0.5761 - val_accuracy: 0.5675 - val_loss: 0.6830\n",
            "Estimated Accuracy: 0.7175\n",
            "95% Confidence Interval for Accuracy: [0.7175, 0.7175]\n",
            "Estimated AUC: 0.7660\n",
            "95% Confidence Interval for AUC: [0.7660, 0.7660]\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "all_train_auc = []  # List to store training AUC for plotting\n",
        "all_val_auc = []  # List to store validation AUC for plotting\n",
        "all_losses = []  # For training loss\n",
        "all_val_losses = []  # For validation loss\n",
        "all_train_acc = []  # For training accuracy\n",
        "all_val_acc = []  # For validation accuracy\n",
        "seeds = []\n",
        "n_simulations = 10 \n",
        "# Monte Carlo Resampling\n",
        "for _ in range(n_simulations):\n",
        "    if os.path.exists('/home/HardDisk/quang_nguyen/quantum_ml/experiment_dropout/quantum_weights3.weights.h5'):\n",
        "        os.remove('/home/HardDisk/quang_nguyen/quantum_ml/experiment_dropout/quantum_weights3.weights.h5')\n",
        "    rd.seed(random)\n",
        "    np.random.seed(random)\n",
        "    tf.random.set_seed(random)\n",
        "    qml.numpy.random.seed(random)\n",
        "    print('Training with random seed:', random)\n",
        "    initial_lr = 0.3\n",
        "    seed_losses = []\n",
        "    seed_val_losses = []\n",
        "    seed_train_acc = []\n",
        "    seed_val_acc = []\n",
        "    seed_train_auc = []  # List to store training AUC for each epoch\n",
        "    seed_val_auc = []  # List to store validation AUC for each epoch\n",
        "\n",
        "    for iteration in range(1, 10):\n",
        "        print(f\"Epochs {iteration}\")\n",
        "\n",
        "        # Switch dropout flag randomly\n",
        "        if rd.random() <= 0.5 and iteration != 1:\n",
        "            drop_flag = True\n",
        "            print(\"Applying Quantum Dropout\")\n",
        "        else:\n",
        "            drop_flag = False\n",
        "            print(\"Not Applying Quantum Dropout\")\n",
        "\n",
        "        # Create the model\n",
        "        model = HybridModel(apply_quantum_dropout=drop_flag)\n",
        "        initial_learning_rate = initial_lr\n",
        "        final_learning_rate = 0.03\n",
        "\n",
        "        # Define learning rate scheduler\n",
        "        lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "            initial_learning_rate=initial_lr,\n",
        "            decay_steps=16,\n",
        "            alpha=final_learning_rate / initial_learning_rate\n",
        "        )\n",
        "\n",
        "        # Adam optimizer with the cosine scheduler\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', 'AUC']\n",
        "        )\n",
        "\n",
        "        lr_logger = LearningRateLogger()\n",
        "\n",
        "        # Train the model with the callback\n",
        "        history = model.fit(\n",
        "            train_images, train_labels,\n",
        "            epochs=1,\n",
        "            batch_size=32,\n",
        "            verbose=1,\n",
        "            validation_data=(test_images, test_labels),\n",
        "            callbacks=[lr_logger]\n",
        "        )\n",
        "\n",
        "        # Update the learning rate for the next iteration\n",
        "        if lr_logger.new_lr:\n",
        "            initial_lr = lr_logger.new_lr\n",
        "\n",
        "        # Store losses and metrics for this iteration\n",
        "        seed_losses.append(history.history['loss'][0])\n",
        "        seed_val_losses.append(history.history['val_loss'][0])\n",
        "        seed_train_acc.append(history.history['accuracy'][0])\n",
        "        seed_val_acc.append(history.history['val_accuracy'][0])\n",
        "        seed_train_auc.append(history.history['AUC'][0])  # Add training AUC value\n",
        "        seed_val_auc.append(history.history['val_AUC'][0])  # Add validation AUC value\n",
        "\n",
        "        # Save the weights after training\n",
        "        model.save_weights('/home/HardDisk/quang_nguyen/quantum_ml/experiment_dropout/quantum_weights3.weights.h5')\n",
        "\n",
        "    # Store the metrics for the seed\n",
        "    seeds.append(random)\n",
        "    all_losses.append(seed_losses)\n",
        "    all_val_losses.append(seed_val_losses)\n",
        "    all_train_acc.append(seed_train_acc)\n",
        "    all_val_acc.append(seed_val_acc)\n",
        "    all_train_auc.append(seed_train_auc)  # Store training AUC\n",
        "    all_val_auc.append(seed_val_auc)  # Store validation AUC\n",
        "\n",
        "accuracies = [max(sublist) for sublist in all_val_acc]\n",
        "aucs = [max(sublist) for sublist in all_val_auc]\n",
        "\n",
        "\n",
        "# Calculate average metrics and 95% confidence intervals\n",
        "mean_accuracy = np.mean(accuracies)\n",
        "ci_accuracy_lower = np.percentile(accuracies, 2.5)\n",
        "ci_accuracy_upper = np.percentile(accuracies, 97.5)\n",
        "\n",
        "mean_auc = np.mean(aucs)\n",
        "ci_auc_lower = np.percentile(aucs, 2.5)\n",
        "ci_auc_upper = np.percentile(aucs, 97.5)\n",
        "\n",
        "# Print results\n",
        "print(f\"Estimated Accuracy: {mean_accuracy:.4f}\")\n",
        "print(f\"95% Confidence Interval for Accuracy: [{ci_accuracy_lower:.4f}, {ci_accuracy_upper:.4f}]\")\n",
        "print(f\"Estimated AUC: {mean_auc:.4f}\")\n",
        "print(f\"95% Confidence Interval for AUC: [{ci_auc_lower:.4f}, {ci_auc_upper:.4f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot Val Loss and Train Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with random seed: 10\n",
            "Epochs 1\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 139ms/step - AUC: 0.5369 - accuracy: 0.5336 - loss: 0.5103 - val_AUC: 0.6787 - val_accuracy: 0.6250 - val_loss: 0.6408\n",
            "Epochs 2\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 132ms/step - AUC: 0.6320 - accuracy: 0.6308 - loss: 0.4824 - val_AUC: 0.7274 - val_accuracy: 0.6675 - val_loss: 0.6161\n",
            "Epochs 3\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 132ms/step - AUC: 0.5902 - accuracy: 0.5839 - loss: 0.3965 - val_AUC: 0.7236 - val_accuracy: 0.6775 - val_loss: 0.6134\n",
            "Epochs 4\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 139ms/step - AUC: 0.6238 - accuracy: 0.6063 - loss: 0.5333 - val_AUC: 0.7298 - val_accuracy: 0.6775 - val_loss: 0.6102\n",
            "Epochs 5\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 142ms/step - AUC: 0.6140 - accuracy: 0.6216 - loss: 0.4358 - val_AUC: 0.7660 - val_accuracy: 0.7175 - val_loss: 0.5765\n",
            "Epochs 6\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 130ms/step - AUC: 0.5209 - accuracy: 0.5058 - loss: 0.5493 - val_AUC: 0.7141 - val_accuracy: 0.6925 - val_loss: 0.6449\n",
            "Epochs 7\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 135ms/step - AUC: 0.6113 - accuracy: 0.6035 - loss: 0.5126 - val_AUC: 0.6831 - val_accuracy: 0.6475 - val_loss: 0.6425\n",
            "Epochs 8\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 128ms/step - AUC: 0.6548 - accuracy: 0.6177 - loss: 0.5305 - val_AUC: 0.7678 - val_accuracy: 0.7350 - val_loss: 0.5700\n",
            "Epochs 9\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 137ms/step - AUC: 0.4767 - accuracy: 0.4790 - loss: 0.4876 - val_AUC: 0.5761 - val_accuracy: 0.5675 - val_loss: 0.6830\n",
            "Epochs 10\n",
            "Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 135ms/step - AUC: 0.6291 - accuracy: 0.6117 - loss: 0.5175 - val_AUC: 0.6236 - val_accuracy: 0.6075 - val_loss: 0.6744\n",
            "Epochs 11\n",
            "Not Applying Quantum Dropout\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - AUC: 0.5670 - accuracy: 0.5660 - loss: 0.5959"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "all_train_auc = []  # List to store training AUC for plotting\n",
        "all_val_auc = []  # List to store validation AUC for plotting\n",
        "all_losses = []  # For training loss\n",
        "all_val_losses = []  # For validation loss\n",
        "all_train_acc = []  # For training accuracy\n",
        "all_val_acc = []  # For validation accuracy\n",
        "seeds = []\n",
        "\n",
        "for random in [10]:\n",
        "    if os.path.exists('/home/HardDisk/quang_nguyen/quantum_ml/experiment_dropout/quantum_weights.weights.h5'):\n",
        "        os.remove('/home/HardDisk/quang_nguyen/quantum_ml/experiment_dropout/quantum_weights.weights.h5')\n",
        "    rd.seed(random)\n",
        "    np.random.seed(random)\n",
        "    tf.random.set_seed(random)\n",
        "    qml.numpy.random.seed(random)\n",
        "    print('Training with random seed:', random)\n",
        "    initial_lr = 0.3\n",
        "    seed_losses = []\n",
        "    seed_val_losses = []\n",
        "    seed_train_acc = []\n",
        "    seed_val_acc = []\n",
        "    seed_train_auc = []  # List to store training AUC for each epoch\n",
        "    seed_val_auc = []  # List to store validation AUC for each epoch\n",
        "\n",
        "    for iteration in range(1, 30):\n",
        "        print(f\"Epochs {iteration}\")\n",
        "\n",
        "        # Switch dropout flag randomly\n",
        "        if rd.random() <= 0.5 and iteration != 1:\n",
        "            drop_flag = True\n",
        "            print(\"Applying Quantum Dropout\")\n",
        "        else:\n",
        "            drop_flag = False\n",
        "            print(\"Not Applying Quantum Dropout\")\n",
        "\n",
        "        # Create the model\n",
        "        model = HybridModel(apply_quantum_dropout=drop_flag)\n",
        "        initial_learning_rate = initial_lr\n",
        "        final_learning_rate = 0.03\n",
        "\n",
        "        # Define learning rate scheduler\n",
        "        lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "            initial_learning_rate=initial_lr,\n",
        "            decay_steps=16,\n",
        "            alpha=final_learning_rate / initial_learning_rate\n",
        "        )\n",
        "\n",
        "        # Adam optimizer with the cosine scheduler\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', 'AUC']\n",
        "        )\n",
        "\n",
        "        lr_logger = LearningRateLogger()\n",
        "\n",
        "        # Train the model with the callback\n",
        "        history = model.fit(\n",
        "            train_images, train_labels,\n",
        "            epochs=1,\n",
        "            batch_size=32,\n",
        "            verbose=1,\n",
        "            validation_data=(test_images, test_labels),\n",
        "            callbacks=[lr_logger]\n",
        "        )\n",
        "\n",
        "        # Update the learning rate for the next iteration\n",
        "        if lr_logger.new_lr:\n",
        "            initial_lr = lr_logger.new_lr\n",
        "\n",
        "        # Store losses and metrics for this iteration\n",
        "        seed_losses.append(history.history['loss'][0])\n",
        "        seed_val_losses.append(history.history['val_loss'][0])\n",
        "        seed_train_acc.append(history.history['accuracy'][0])\n",
        "        seed_val_acc.append(history.history['val_accuracy'][0])\n",
        "        seed_train_auc.append(history.history['AUC'][0])  # Add training AUC value\n",
        "        seed_val_auc.append(history.history['val_AUC'][0])  # Add validation AUC value\n",
        "\n",
        "        # Save the weights after training\n",
        "        model.save_weights('/home/HardDisk/quang_nguyen/quantum_ml/experiment_dropout/quantum_weights.weights.h5')\n",
        "\n",
        "    # Store the metrics for the seed\n",
        "    seeds.append(random)\n",
        "    all_losses.append(seed_losses)\n",
        "    all_val_losses.append(seed_val_losses)\n",
        "    all_train_acc.append(seed_train_acc)\n",
        "    all_val_acc.append(seed_val_acc)\n",
        "    all_train_auc.append(seed_train_auc)  # Store training AUC\n",
        "    all_val_auc.append(seed_val_auc)  # Store validation AUC\n",
        "\n",
        "# Plotting for each seed\n",
        "epochs = range(1, 30)\n",
        "\n",
        "for i, seed in enumerate(seeds):\n",
        "    plt.figure(figsize=(12, 12))\n",
        "\n",
        "    # Loss plot for the current seed\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(epochs, all_losses[i], label='Train Loss')\n",
        "    plt.plot(epochs, all_val_losses[i], linestyle='--', label='Test Loss')\n",
        "    plt.title(f'Loss per Epoch (Seed {seed})')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    # Accuracy plot for the current seed\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(epochs, all_train_acc[i], label='Train Acc')\n",
        "    plt.plot(epochs, all_val_acc[i], linestyle='--', label='Val Acc')\n",
        "    plt.title(f'Accuracy per Epoch (Seed {seed})')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    # AUC plot for the current seed\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.plot(epochs, all_train_auc[i], label='Train AUC')\n",
        "    plt.plot(epochs, all_val_auc[i], linestyle='--', label='Val AUC')\n",
        "    plt.title(f'AUC per Epoch (Seed {seed})')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('AUC')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "uXpH5753pfZU"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Pennylane",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
